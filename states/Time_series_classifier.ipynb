{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "636f10b6-3b57-4078-9957-bf193c50fa98",
   "metadata": {},
   "source": [
    "# **Train and evaluate a time series classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31bb150-c3c9-4328-beaa-919b5674876a",
   "metadata": {},
   "source": [
    "# 1) Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d6c11-a061-4608-a27b-d75600920398",
   "metadata": {},
   "source": [
    "We need to chop the data into chunks of a specified length (e.g. 30s intervals), while making sure that the intervals are not interrupted by excluded bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c5313b-f1e5-4bf9-9632-3a72d2af35aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies from sktime environment:\n",
    "\n",
    "from __future__ import print_function\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8ed18f-645b-4aca-8545-71eaf6331e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'States_ceiling_reduced.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Rename first column to \"Data_idx\"\n",
    "l_colums = list(df.columns)\n",
    "l_colums[0] = 'Data_idx'\n",
    "df.columns = l_colums\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8391db04-2529-492c-b4c7-437609bdb710",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ecd65-5baf-4b18-b7e8-b6eab5c92ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long [in seconds] does the interval have to be?\n",
    "interval_size_s = 20\n",
    "\n",
    "# Create column that indicates for each bin, whether the bins for the following 'interval_size_s'-seconds exist\n",
    "df['next_{}s_clean'.format(str(interval_size_s))] = False\n",
    "interval_size_bins = interval_size_s*4\n",
    "\n",
    "# These computations have to be done on a per session level\n",
    "time_start = time.time()\n",
    "for mouse in df['Animal_ID'].unique():\n",
    "    for session in df.loc[df['Animal_ID'] == mouse, 'Session'].unique():\n",
    "        df_temp = df.loc[(df['Animal_ID'] == mouse) & (df['Session'] == session)].copy()\n",
    "        \n",
    "        # We don´t have to check the last 'interval_size_bins'-bins for each session, since they can´t have enough clean bins following\n",
    "        for row in range(df_temp.shape[0]-interval_size_bins):         \n",
    "            # Get the bin count of the current row\n",
    "            start_bin = df_temp.iloc[row]['Bin']\n",
    "            # Get the bin count of that row, that is 'interval_size_bins'-rows afterwards\n",
    "            last_bin = df_temp.iloc[row + interval_size_bins]['Bin']\n",
    "            # Does the difference between the two match interval_size_bins? If not, last_bin was too large since there were some bins missing inbetween\n",
    "            if last_bin - start_bin == interval_size_bins:\n",
    "                # If matching, mark the corresponding bin as True in the original df\n",
    "                df.loc[(df['Animal_ID'] == mouse) & (df['Session'] == session)\n",
    "                            & (df['Bin'] == start_bin), 'next_{}s_clean'.format(str(interval_size_s))] = True\n",
    "                \n",
    "    print('Done with {}\\nTime elapsed so far: {} seconds\\n---------------------------------'.format(mouse, time.time()-time_start))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb32a8e-936f-4d9c-9ded-adb3ddf68738",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['{}s_interval_ID'.format(str(interval_size_s))] = -1\n",
    "interval_id_count = 0\n",
    "\n",
    "for mouse in df['Animal_ID'].unique():\n",
    "    for session in df.loc[df['Animal_ID'] == mouse, 'Session'].unique():\n",
    "        # Initial check, whether intervals are present to begin the while loop:\n",
    "        intervals_present = df.loc[(df['Animal_ID'] == mouse) & (df['Session'] == session)\n",
    "                                & (df['next_{}s_clean'.format(str(interval_size_s))] == True) \n",
    "                                & (df['{}s_interval_ID'.format(str(interval_size_s))] == -1)].shape[0] > 0\n",
    "        \n",
    "        while intervals_present == True:\n",
    "            # Find the next bin that was already identified as potential start_bin (next_Xs_clean == True) \n",
    "            # and which is not yet included in an interval (Xs_interval_ID == -1)\n",
    "            idx_start_bin = df.loc[(df['Animal_ID'] == mouse) & (df['Session'] == session)\n",
    "                                    & (df['next_{}s_clean'.format(str(interval_size_s))] == True) \n",
    "                                    & (df['{}s_interval_ID'.format(str(interval_size_s))] == -1)].index[0]\n",
    "\n",
    "            # Starting from this bin, mark the corresponding bins (interval_size_bin) with one interval ID:\n",
    "            # First, identify the position of that index in the index list to find the index of the last_bin of that interval\n",
    "            position_idx_start_bin = df.index.to_list().index(idx_start_bin)\n",
    "\n",
    "            idx_last_bin = df.index.to_list()[position_idx_start_bin + interval_size_bins - 1]\n",
    "\n",
    "            df.loc[idx_start_bin : idx_last_bin, '{}s_interval_ID'.format(str(interval_size_s))] = interval_id_count\n",
    "            \n",
    "            interval_id_count = interval_id_count + 1\n",
    "            \n",
    "            # Update intervals_present:\n",
    "            intervals_present = df.loc[(df['Animal_ID'] == mouse) & (df['Session'] == session)\n",
    "                                    & (df['next_{}s_clean'.format(str(interval_size_s))] == True) \n",
    "                                    & (df['{}s_interval_ID'.format(str(interval_size_s))] == -1)].shape[0] > 0\n",
    "\n",
    "\n",
    "\n",
    "print('Congratulations, you found {} intervals!'.format(df['{}s_interval_ID'.format(str(interval_size_s))].unique().shape[0] - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a44aaf-b603-44b5-b09e-b89e3e8d5434",
   "metadata": {},
   "source": [
    "## Some sanity checks:\n",
    "\n",
    "#### Are there multiple mice in any interval? Corresponding IDs will show up in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba6f4b-d5e4-4892-b377-6693829534be",
   "metadata": {},
   "outputs": [],
   "source": [
    "[elem for elem in df['{}s_interval_ID'.format(str(interval_size_s))].unique() if df.loc[df['{}s_interval_ID'.format(str(interval_size_s))] == elem, 'Animal_ID'].unique().shape[0] != 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982fb6db-ea26-44d4-a5ae-b75d12487dbc",
   "metadata": {},
   "source": [
    "#### Are there multiple sessions in any interval? Corresponding IDs will show up in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a8c8a5-870d-40a8-9b40-98fc6f0826aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "[elem for elem in df['{}s_interval_ID'.format(str(interval_size_s))].unique() if df.loc[df['{}s_interval_ID'.format(str(interval_size_s))] == elem, 'Session'].unique().shape[0] != 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68346641-d404-4027-99f7-9d77d7f88dc1",
   "metadata": {},
   "source": [
    "#### Are there any intervals that don´t have the correct length? Corresponding IDs will show up in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3c31a-a64e-487a-be4d-6712d44e1644",
   "metadata": {},
   "outputs": [],
   "source": [
    "[elem for elem in df['{}s_interval_ID'.format(str(interval_size_s))].unique() if df.loc[df['{}s_interval_ID'.format(str(interval_size_s))] == elem].shape[0] != interval_size_bins]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d4d657-46df-47c7-ad64-b74d0837cdad",
   "metadata": {},
   "source": [
    "## Alright, then let´s finish the pre-processing with some final steps: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d2aaca-1efd-4918-98a9-83d3408db579",
   "metadata": {},
   "source": [
    "#### Add \"early-mid-late\" labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade88736-4869-4d38-ada1-e9595fcec0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_session_times = {}\n",
    "\n",
    "# For each session type, get the maximum time and use that to define the borders between early-mid-late chunks:\n",
    "for session in df['Session'].unique():\n",
    "    d_session_times[session] = {}\n",
    "    d_session_times[session]['max_session_time'] = df.loc[df['Session'] == session, 'Times'].max()\n",
    "    d_session_times[session]['early-mid'] = round(d_session_times[session]['max_session_time'] / 3, 0)\n",
    "    d_session_times[session]['mid-late'] = round(d_session_times[session]['max_session_time'] / 3 * 2, 0)\n",
    "\n",
    "df['session_time_label'] = ''\n",
    "# For each session type, assign the corresponding label to each bin\n",
    "for session in df['Session'].unique():\n",
    "    # Mark as early:\n",
    "    df.loc[(df['Session'] == session) & (df['Times'] < d_session_times[session]['early-mid']), 'session_time_label'] = 'early'\n",
    "    # Mark as mid:\n",
    "    df.loc[(df['Session'] == session) & (df['Times'] >= d_session_times[session]['early-mid']) \n",
    "                & (df_test['Times'] < d_session_times[session]['mid-late']), 'session_time_label'] = 'mid'\n",
    "    # Mark as late:\n",
    "    df.loc[(df_test['Session'] == session) & (df['Times'] >= d_session_times[session]['mid-late']), 'session_time_label'] = 'late'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e5e2ab-a826-4efc-b784-5830f9f9382b",
   "metadata": {},
   "source": [
    "### Visualize the distribution of all labels to check for a ~balanced distribution (also across mice):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaf42bf-cde2-40b9-bb59-0ab627357f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_interval_counts = {'Animal_ID': [],\n",
    "                     'Session': [],\n",
    "                     'session_time_label': [],\n",
    "                     'interval_count': []}\n",
    "\n",
    "for mouse in df['Animal_ID'].unique():\n",
    "    for session in df.loc[df['Animal_ID'] == mouse, 'Session'].unique():\n",
    "        for session_time_label in df.loc[(df['Animal_ID'] == mouse) & (df['Session'] == session), 'session_time_label'].unique():\n",
    "            d_interval_counts['Animal_ID'].append(mouse)\n",
    "            d_interval_counts['Session'].append(session)\n",
    "            d_interval_counts['session_time_label'].append(session_time_label)\n",
    "            interval_count = len([elem for elem in \n",
    "                                  df.loc[(df['Animal_ID'] == mouse) & (df['Session'] == session) & (df['session_time_label'] == session_time_label), '{}s_interval_ID'.format(str(interval_size_s))].unique()\n",
    "                                  if elem != -1])\n",
    "            d_interval_counts['interval_count'].append(interval_count)\n",
    "        \n",
    "df_interval_counts = pd.DataFrame(data=d_interval_counts)\n",
    "\n",
    "fig = plt.figure(figsize=(20,12), facecolor='white')\n",
    "gs = fig.add_gridspec(2,2)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "sns.stripplot(data=df_interval_counts.loc[df_interval_counts['Session'] == 'OF'], x='session_time_label', y='interval_count', hue='Animal_ID', palette='Spectral', ax=ax1, size=8, dodge=True)\n",
    "plt.title('OF', fontsize=18)\n",
    "ax1.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0,1], sharey=ax1)\n",
    "sns.stripplot(data=df_interval_counts.loc[df_interval_counts['Session'] == 'EPM'], x='session_time_label', y='interval_count', hue='Animal_ID', palette='Spectral', ax=ax2, size=8, dodge=True)\n",
    "plt.title('EPM', fontsize=18)\n",
    "ax2.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "ax3 = fig.add_subplot(gs[1,0], sharey=ax1)\n",
    "sns.stripplot(data=df_interval_counts.loc[df_interval_counts['Session'] == 'CD1'], x='session_time_label', y='interval_count', hue='Animal_ID', palette='Spectral', ax=ax3, size=8, dodge=True)\n",
    "plt.title('CD1', fontsize=18)\n",
    "ax3.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1,1], sharey=ax1)\n",
    "sns.stripplot(data=df_interval_counts.loc[df_interval_counts['Session'] == 'CD2'], x='session_time_label', y='interval_count', hue='Animal_ID', palette='Spectral', ax=ax4, size=8, dodge=True)\n",
    "plt.title('CD2', fontsize=18)\n",
    "ax4.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# As alternative:\n",
    "# sns.catplot(data=df_interval_counts, x='session_time_label', y='interval_count', col='Session', hue='Animal_ID', palette='Spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896dd236-9510-41a8-9204-0d73550a5a4e",
   "metadata": {},
   "source": [
    "### Add combo-classification-labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e65bf-73c3-4e03-90d8-1466318edc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['true_classification_labels'] = df['session_time_label'] + '_' + df['Session']\n",
    "\n",
    "# Print labels to check whether it worked:\n",
    "df['true_classification_labels'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26557e-cded-49ac-97f1-b3f3780d3f5b",
   "metadata": {},
   "source": [
    "### Add train-test-split labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab028600-f64f-403b-b858-f28c844751af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraction of data that should be used for testing:\n",
    "test_fraction = 0.1\n",
    "\n",
    "\n",
    "df['train_test_split_labels'] = ''\n",
    "\n",
    "for classification_label in df['true_classification_labels'].unique():\n",
    "    l_interval_ids = [elem for elem in df.loc[df['true_classification_labels'] == classification_label, '{}s_interval_ID'.format(str(interval_size_s))].unique() if elem != -1]\n",
    "    interval_count = len(l_interval_ids)\n",
    "    l_test_ids = random.sample(l_interval_ids, int(test_fraction*interval_count) + 1)\n",
    "    l_train_ids = [elem for elem in l_interval_ids if elem not in l_test_ids]\n",
    "    \n",
    "    df.loc[df['{}s_interval_ID'.format(str(interval_size_s))].isin(l_test_ids), 'train_test_split_labels'] = 'test'\n",
    "    df.loc[df['{}s_interval_ID'.format(str(interval_size_s))].isin(l_train_ids), 'train_test_split_labels'] = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f9c0d-7a40-4007-a8f3-1bed1e053c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_usable_bins = df.loc[df['{}s_interval_ID'.format(str(interval_size_s))] != -1].shape[0]\n",
    "train_bins = df.loc[df['train_test_split_labels'] == 'train'].shape[0]\n",
    "test_bins = df.loc[df['train_test_split_labels'] == 'test'].shape[0]\n",
    "\n",
    "if train_bins + test_bins == total_usable_bins:\n",
    "    print('Overall, we can use {}% of all bins for the time-series classifier'.format(round(total_usable_bins / df.shape[0] * 100, 2)))\n",
    "    print('{}% of these usable bins (and {} of all bins) will be used for training'.format(round(train_bins/total_usable_bins * 100, 2), round(train_bins / df.shape[0] * 100, 2)))\n",
    "    print('Consequently, the remaining {}% of the usable bins (and {} of all bins) will be used for testing'.format(round(test_bins/total_usable_bins * 100, 2), round(test_bins / df.shape[0] * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c157ae-3c55-48e2-aac0-425783ef8b64",
   "metadata": {},
   "source": [
    "## All pre-processing is done. Let´s save the data and continue with training of the classifiers! :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd33425-5082-492f-9e45-862ca3e7b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('States_ceiling_reduced_for_classification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e5e01a-0c5f-4c9b-8ff9-30da6d5ef96f",
   "metadata": {},
   "source": [
    "# 2) Train & evaluate the classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa57295-cfa7-477f-852d-9c2864cd3cbe",
   "metadata": {},
   "source": [
    "#### If training was already performed just load the dependencies and set `train_and_evaluate_the_classifiers` to `False` to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a03bafd-fd41-4916-90f9-db9b19a3c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies from sktime environment:\n",
    "\n",
    "from __future__ import print_function\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from statistics import stdev, mean\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sktime.datasets import load_arrow_head  # univariate dataset\n",
    "from sktime.datasets.base import load_basic_motions  # multivariate dataset\n",
    "from sktime.transformations.panel.rocket import MiniRocket, MiniRocketMultivariate\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae9d838-c9ca-46f3-bd02-b71ec3f2d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to 'True' if training shall be performed from scratch & remember to also specify the number of runs\n",
    "train_and_evaluate_the_classifiers = False\n",
    "\n",
    "# If you want to train another round of classifiers, how many repitions do you wish to compute?\n",
    "total_runs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d3b31-6ba0-4889-a0d8-514dfb93cd7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interval_size_s = 20\n",
    "\n",
    "df = pd.read_csv('States_ceiling_reduced_for_classification.csv', index_col = 0)\n",
    "df_umap_results = pd.read_csv('States_ceiling_reduced2_with_UMAP.csv', index_col = 0)\n",
    "df_umap_results = df_umap_results[['Data_idx', 'UMAP_cont_r1_1', 'UMAP_cont_r1_2', 'UMAP_cont_r2_1', 'UMAP_cont_r2_2', 'UMAP_cont_r3_1', 'UMAP_cont_r3_2']]\n",
    "df = pd.merge(df, df_umap_results, on='Data_idx', how='outer')\n",
    "\n",
    "if train_and_evaluate_the_classifiers == False:\n",
    "    with open('classifier_results_{}s_intervals.p'.format(str(interval_size_s)), 'rb') as fp:\n",
    "        d_classification_results = pickle.load(fp)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af635e1-e7ec-41f8-a25e-af110895a91e",
   "metadata": {},
   "source": [
    "### Train and score the classifiers. This will only be executed, if `train_and_evaluate_the_classifiers` is `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75585cc-8534-4a7d-83fe-84ff373a87ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_and_evaluate_the_classifiers:\n",
    "\n",
    "    # List of tuples that specify the respective approaches that will be used to train classifiers\n",
    "    # First element of the tuple is just a descriptive name\n",
    "    # Second element of the tuple is a list of all dimensions that will be used as timeseries input for the classifiers\n",
    "    # If the list of dimensions contains more than just one dimension, a multivariate MiniRocket will be used\n",
    "    # In addition, there are also some approaches that include the string \"_shuffled_\"\n",
    "    # This will be detected by an if-condition and cause random shuffling of the classification labels in the training dataset\n",
    "    l_approaches = [('norm_HeartRate', ['norm_HeartRate']),\n",
    "                     ('norm_HR_Med_Delta', ['norm_HR_Med_Delta']),\n",
    "                     ('norm_HR_High_Amp', ['norm_HR_High_Amp']),\n",
    "                     ('norm_Ceiling', ['norm_Ceiling']),\n",
    "                     ('norm_DistanceToCeiling', ['norm_DistanceToCeiling']),\n",
    "                     ('norm_Motion', ['norm_Motion']),\n",
    "                     ('norm_AreaExplored', ['norm_AreaExplored']),\n",
    "                     ('norm_Speed', ['norm_Speed']), \n",
    "                     ('norm_Temperature', ['norm_Temperature']), \n",
    "                     ('all_dimensions', ['norm_HeartRate', 'norm_HR_Med_Delta', 'norm_HR_High_Amp', 'norm_Ceiling', 'norm_DistanceToCeiling',\n",
    "                                         'norm_Motion', 'norm_AreaExplored', 'norm_Speed',\n",
    "                                         'norm_Temperature']),\n",
    "                     ('UMAP1', ['UMAP_cont_r1_1', 'UMAP_cont_r1_2']),\n",
    "                     ('UMAP2', ['UMAP_cont_r2_1', 'UMAP_cont_r2_2']), \n",
    "                     ('UMAP3', ['UMAP_cont_r3_1', 'UMAP_cont_r3_2']),\n",
    "                     ('univariate_shuffled_1', ['norm_HeartRate']),\n",
    "                     ('univariate_shuffled_2', ['norm_Ceiling']),\n",
    "                     ('multivariate_shuffled_all_dimensions', ['norm_HeartRate', 'norm_HR_Med_Delta', 'norm_HR_High_Amp', 'norm_Ceiling', 'norm_DistanceToCeiling',\n",
    "                                                               'norm_Motion', 'norm_AreaExplored', 'norm_Speed', \n",
    "                                                               'norm_Temperature']),\n",
    "                     ('multivariate_shuffled_UMAP', ['UMAP_cont_r1_1', 'UMAP_cont_r1_2'])]\n",
    "\n",
    "\n",
    "    # Setup the dictionary that will be fed with all required data and results:\n",
    "    d_classification_results = {'Results': {'Scores': {'Approach': [], \n",
    "                                                       'Run': [], \n",
    "                                                       'Score': [],\n",
    "                                                       'Score_only_times': [],\n",
    "                                                       'Score_only_sessions': []},\n",
    "                                           'DataFrames': {'Approach': [],\n",
    "                                                          'Run': [],\n",
    "                                                          'DataFrame': []}}}\n",
    "\n",
    "    # For validation, all computations will be run five times:\n",
    "    for run in range(1,total_runs+1):\n",
    "        # Make indiviual dictionaries for each run\n",
    "        d_classification_results[run] = {}\n",
    "        for approach in l_approaches:\n",
    "            # Make individual dictionaries for each approach in each run\n",
    "            d_classification_results[run][approach[0]] = {}\n",
    "\n",
    "            # Now setup the individual training and test data dictionaries for each approach (number of dimensions varies)\n",
    "            for train_test_key in ['train', 'test']:\n",
    "                d_classification_results[run][approach[0]][train_test_key] = {}\n",
    "\n",
    "                for dimension in approach[1]:\n",
    "                    d_classification_results[run][approach[0]][train_test_key]['dim_{}'.format(str(approach[1].index(dimension)).zfill(2))] = []\n",
    "\n",
    "                for metadata_col in ['Animal_ID', 'Session', 'session_time_label', 'true_classification_label', '{}s_interval_ID'.format(str(interval_size_s))]:\n",
    "                    d_classification_results[run][approach[0]][train_test_key][metadata_col] = []\n",
    "\n",
    "            # Add the respective data (while split remains the same for all, the timeseries data varies of course)\n",
    "            l_interval_ids = [elem for elem in df['{}s_interval_ID'.format(str(interval_size_s))].unique() if elem != -1]\n",
    "            for interval_id in l_interval_ids:\n",
    "                # To make things easier, specify a temporary DataFrame\n",
    "                df_temp = df.loc[df['{}s_interval_ID'.format(str(interval_size_s))] == interval_id].copy()\n",
    "\n",
    "                # Get the information whether this timeseries was assigned to the training or to the test data\n",
    "                train_test_split_label = df_temp['train_test_split_labels'].unique()[0]\n",
    "\n",
    "                # Get the respective timeseries data\n",
    "                for dimension in approach[1]:\n",
    "                    timeseries = pd.Series(df_temp[dimension].values)\n",
    "                    d_classification_results[run][approach[0]][train_test_split_label]['dim_{}'.format(str(approach[1].index(dimension)).zfill(2))].append(timeseries)\n",
    "\n",
    "                # Get all remaining metadata that might be of relevance:\n",
    "                Animal_ID = df_temp['Animal_ID'].unique()[0]\n",
    "                Session = df_temp['Session'].unique()[0]\n",
    "                session_time_label = df_temp['session_time_label'].unique()[0]\n",
    "\n",
    "                # Get the desired classification label\n",
    "                true_classification_label = df_temp['true_classification_labels'].unique()[0]\n",
    "\n",
    "                # Append all these information to the respective lists for easy conversion in a DataFrame object\n",
    "                d_classification_results[run][approach[0]][train_test_split_label]['Animal_ID'].append(Animal_ID)\n",
    "                d_classification_results[run][approach[0]][train_test_split_label]['Session'].append(Session)\n",
    "                d_classification_results[run][approach[0]][train_test_split_label]['session_time_label'].append(session_time_label)\n",
    "                d_classification_results[run][approach[0]][train_test_split_label]['true_classification_label'].append(true_classification_label)\n",
    "                d_classification_results[run][approach[0]][train_test_split_label]['{}s_interval_ID'.format(str(interval_size_s))].append(interval_id)\n",
    "\n",
    "            # Convert the collected data to the correct format that is required for the training & testing of the classifiers\n",
    "            df_train_w_meta = pd.DataFrame(data=d_classification_results[run][approach[0]]['train'])\n",
    "            l_dims = [elem for elem in df_train_w_meta.columns if elem.startswith('dim_')]\n",
    "            df_x_train = df_train_w_meta[l_dims]\n",
    "            np_y_train = df_train_w_meta['true_classification_label'].values\n",
    "\n",
    "            # If approach contains the string \"shuffled\", the corresponding classification labels of the training dataset will be shuffled\n",
    "            if 'shuffled' in approach[0]:\n",
    "                random.shuffle(np_y_train)\n",
    "\n",
    "            # Same data extraction for the test dataset (no shuffling of classification labels for the test data)\n",
    "            df_test_w_meta = pd.DataFrame(data=d_classification_results[run][approach[0]]['test'])\n",
    "            df_x_test = df_test_w_meta[l_dims]\n",
    "            np_y_test = df_test_w_meta['true_classification_label'].values\n",
    "\n",
    "            # Use MiniRocketMultivariate if more than one dimension as input\n",
    "            if len(l_dims) > 1:\n",
    "                minirocket_type = 'multivariate'\n",
    "                minirocket_multi = MiniRocketMultivariate()\n",
    "                minirocket_multi.fit(df_x_train)\n",
    "                df_x_train_transform = minirocket_multi.transform(df_x_train)\n",
    "                df_x_test_transform = minirocket_multi.transform(df_x_test)\n",
    "            # Use univariate version if only a single dimension is used as input\n",
    "            else:\n",
    "                minirocket_type = 'univariate'\n",
    "                minirocket = MiniRocket()\n",
    "                minirocket.fit(df_x_train)\n",
    "                df_x_train_transform = minirocket.transform(df_x_train)\n",
    "                df_x_test_transform = minirocket.transform(df_x_test)\n",
    "\n",
    "\n",
    "            # Setup and train the classifier:\n",
    "            classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True, class_weight='balanced')\n",
    "            classifier.fit(df_x_train_transform, np_y_train)\n",
    "\n",
    "            # Predict the labels of the test set and store the DataFrame in the dictionary:\n",
    "            df_test_w_meta['predicted_labels'] = classifier.predict(df_x_test_transform)\n",
    "            df_test_w_meta['predicted_time'] = [elem[:elem.index('_')] for elem in classifier.predict(df_x_test_transform)]\n",
    "            df_test_w_meta['predicted_session'] = [elem[elem.index('_') + 1:] for elem in classifier.predict(df_x_test_transform)]\n",
    "            d_classification_results['Results']['DataFrames']['Approach'].append(approach[0])\n",
    "            d_classification_results['Results']['DataFrames']['Run'].append(run)\n",
    "            d_classification_results['Results']['DataFrames']['DataFrame'].append(df_test_w_meta)\n",
    "\n",
    "\n",
    "            # Score the classifier:\n",
    "\n",
    "            # 1 - Entire classification label (time + session):\n",
    "            score = classifier.score(df_x_test_transform, np_y_test)\n",
    "            print('The {}-classifier reached a score of: {} & {} MiniRocket was used'.format(approach[0], round(score,4), minirocket_type))\n",
    "            #d_classification_results[run][approach[0]]['Score'] = score\n",
    "\n",
    "            # 2 - Only times:\n",
    "            time_score = df_test_w_meta['session_time_label'].eq(df_test_w_meta['predicted_time']).value_counts()[True] / df_test_w_meta.shape[0]\n",
    "\n",
    "            # 3 - Only session:\n",
    "            session_score = df_test_w_meta['Session'].eq(df_test_w_meta['predicted_session']).value_counts()[True] / df_test_w_meta.shape[0]\n",
    "\n",
    "\n",
    "            # Also add all information to the 'Results' part of the main dictionary, so that it can directly be transformed into a DataFrame once all computations are done:\n",
    "            d_classification_results['Results']['Scores']['Approach'].append(approach[0])\n",
    "            d_classification_results['Results']['Scores']['Run'].append(run)\n",
    "            d_classification_results['Results']['Scores']['Score'].append(score)\n",
    "            d_classification_results['Results']['Scores']['Score_only_times'].append(time_score)\n",
    "            d_classification_results['Results']['Scores']['Score_only_sessions'].append(session_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b4e89d-3327-4d54-90ba-7d8bc89daf69",
   "metadata": {},
   "source": [
    "## This is the evaluation section of the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438eba9c-941a-4006-92b6-84574873ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classifier_results = pd.DataFrame(data=d_classification_results['Results']['Scores'])\n",
    "\n",
    "fig = plt.figure(figsize=(25,10), facecolor='white')\n",
    "gs = fig.add_gridspec(1, 3)\n",
    "\n",
    "fig.add_subplot(gs[0,0])\n",
    "sns.stripplot(data=df_classifier_results, x='Approach', y='Score_only_times')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Score on time labels only', fontsize=18)\n",
    "\n",
    "fig.add_subplot(gs[0,1])\n",
    "sns.stripplot(data=df_classifier_results, x='Approach', y='Score_only_sessions')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Score on session labels only', fontsize=18)\n",
    "\n",
    "fig.add_subplot(gs[0,2])\n",
    "sns.stripplot(data=df_classifier_results, x='Approach', y='Score')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Score on entire labels', fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Score_distributions.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f549711-484a-4c3c-9a36-7afa04c672c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_label_order = ['early_OF', 'mid_OF', 'late_OF',\n",
    "                'early_EPM', 'mid_EPM', 'late_EPM', \n",
    "                'early_CD1', 'mid_CD1', 'late_CD1', \n",
    "                'early_CD2', 'mid_CD2', 'late_CD2']\n",
    "\n",
    "l_approaches = ['norm_HeartRate', 'norm_Ceiling', 'all_dimensions', 'UMAP3', 'shuffled']\n",
    "\n",
    "rows = len(l_approaches)\n",
    "columns = 3 #len(set(d_classification_results['Results']['DataFrames']['Run']))\n",
    "\n",
    "fig = plt.figure(figsize=(20, 6*rows), facecolor='white')\n",
    "gs = fig.add_gridspec(rows, columns)\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for approach in l_approaches:\n",
    "    row = l_approaches.index(approach)\n",
    "    \n",
    "    if approach == 'shuffled':\n",
    "        l_indices = [index for index, element in enumerate(d_classification_results['Results']['DataFrames']['Approach']) if approach in element]\n",
    "        column = 0\n",
    "        \n",
    "        for index in l_indices:\n",
    "            if column < 3:\n",
    "                df_temp = d_classification_results['Results']['DataFrames']['DataFrame'][index].copy()\n",
    "                df_confusion = pd.crosstab(df_temp['true_classification_label'], df_temp['predicted_labels'])\n",
    "                try: \n",
    "                    # Try whether all labels are present, if not, one of the following two lines will throw an error and we continue with the next shuffled predictions until we have 3\n",
    "                    df_confusion = df_confusion.reindex(index=l_label_order)\n",
    "                    df_confusion[l_label_order]\n",
    "\n",
    "                    # Plot the data:\n",
    "                    fig.add_subplot(gs[row, column])\n",
    "                    sns.heatmap(df_confusion, annot=True)\n",
    "                    full_approach_name = d_classification_results['Results']['DataFrames']['Approach'][index]\n",
    "                    plt.title('{} - Run: {}'.format(full_approach_name, column+1))\n",
    "\n",
    "                    # Increase column count\n",
    "                    column = column + 1\n",
    "                except:\n",
    "                    continue  \n",
    "    \n",
    "    else:\n",
    "        l_indices = [index for index, element in enumerate(d_classification_results['Results']['DataFrames']['Approach']) if element == approach]\n",
    "        for index in l_indices:\n",
    "            column = l_indices.index(index)\n",
    "            if column < 3:\n",
    "                df_temp = d_classification_results['Results']['DataFrames']['DataFrame'][index].copy()\n",
    "                df_confusion = pd.crosstab(df_temp['true_classification_label'], df_temp['predicted_labels'])\n",
    "\n",
    "                # Sort the confusion matrix:\n",
    "                df_confusion = df_confusion.reindex(index=l_label_order)\n",
    "                df_confusion = df_confusion[l_label_order]\n",
    "\n",
    "                # Plot the data:\n",
    "                fig.add_subplot(gs[row, column])\n",
    "                sns.heatmap(df_confusion, annot=True)\n",
    "                plt.title('{} - Run: {}'.format(approach, column+1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Confusion_matrices.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf685406-e684-4203-a256-0418313d0860",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_and_evaluate_the_classifiers:\n",
    "    l_approaches_main = [('norm_HeartRate', ['norm_HeartRate']),\n",
    "                         ('norm_HR_Med_Delta', ['norm_HR_Med_Delta']),\n",
    "                         ('norm_HR_High_Amp', ['norm_HR_High_Amp']),\n",
    "                         ('norm_Ceiling', ['norm_Ceiling']),\n",
    "                         ('norm_DistanceToCeiling', ['norm_DistanceToCeiling']),\n",
    "                         ('norm_Motion', ['norm_Motion']),\n",
    "                         ('norm_AreaExplored', ['norm_AreaExplored']),\n",
    "                         ('norm_Speed', ['norm_Speed']), \n",
    "                         ('norm_Temperature', ['norm_Temperature']), \n",
    "                         ('all_dimensions', ['norm_HeartRate', 'norm_HR_Med_Delta', 'norm_HR_High_Amp', 'norm_Ceiling', 'norm_DistanceToCeiling',\n",
    "                                             'norm_Motion', 'norm_AreaExplored', 'norm_Speed',\n",
    "                                             'norm_Temperature']),\n",
    "                         ('UMAP1', ['UMAP_cont_r1_1', 'UMAP_cont_r1_2']),\n",
    "                         ('UMAP2', ['UMAP_cont_r2_1', 'UMAP_cont_r2_2']), \n",
    "                         ('UMAP3', ['UMAP_cont_r3_1', 'UMAP_cont_r3_2']),\n",
    "                         ('univariate_shuffled_1', ['norm_HeartRate']),\n",
    "                         ('univariate_shuffled_2', ['norm_Ceiling']),\n",
    "                         ('multivariate_shuffled_all_dimensions', ['norm_HeartRate', 'norm_HR_Med_Delta', 'norm_HR_High_Amp', 'norm_Ceiling', 'norm_DistanceToCeiling',\n",
    "                                                                   'norm_Motion', 'norm_AreaExplored', 'norm_Speed', \n",
    "                                                                   'norm_Temperature']),\n",
    "                         ('multivariate_shuffled_UMAP', ['UMAP_cont_r1_1', 'UMAP_cont_r1_2'])]\n",
    "\n",
    "    l_approaches = [elem[0] for elem in l_approaches_main]\n",
    "\n",
    "\n",
    "    d_classification_results['Results']['Errors'] = {'Summary': {}, \n",
    "                                                     'Individual': {}}\n",
    "\n",
    "    #l_approaches = set(d_classification_results['Results']['DataFrames']['Approach'])\n",
    "\n",
    "    for approach in l_approaches:\n",
    "        # Create a temporary dictionary that will be added to the main results dict if we have some results to compute:\n",
    "        d_temp = {'expl_true': [],\n",
    "                 'expl_within': [],\n",
    "                 'expl_between': [],\n",
    "                 'cond_true': [],\n",
    "                 'cond_within': [],\n",
    "                 'cond_between': [],\n",
    "                 'total_true': [],\n",
    "                 'total_within': [],\n",
    "                 'total_between': []\n",
    "                 }    \n",
    "\n",
    "        # Identify the indices of all runs of that approach:\n",
    "        l_indices = [index for index, element in enumerate(d_classification_results['Results']['DataFrames']['Approach']) if approach == element]\n",
    "        for index in l_indices:\n",
    "            run = l_indices.index(index)+1\n",
    "            try: \n",
    "                df_temp = d_classification_results['Results']['DataFrames']['DataFrame'][index].copy()\n",
    "                df_confusion = pd.crosstab(df_temp['true_classification_label'], df_temp['predicted_labels'])\n",
    "\n",
    "                # Sort the confusion matrix:\n",
    "                df_confusion = df_confusion.reindex(index=l_label_order)\n",
    "                df_confusion = df_confusion[l_label_order]\n",
    "\n",
    "                # How many expl are correctly predicted as expl?\n",
    "                correct_expl = 0\n",
    "                for label in l_label_order[:6]:\n",
    "                    correct_expl = correct_expl + df_confusion.loc[label, label]\n",
    "\n",
    "                # How many cond are correctly predicted as cond?\n",
    "                correct_cond = 0\n",
    "                for label in l_label_order[6:]:\n",
    "                    correct_cond = correct_cond + df_confusion.loc[label, label]\n",
    "\n",
    "                # How many total correct predictions?\n",
    "                total_correct = 0\n",
    "                for label in l_label_order:\n",
    "                    total_correct = total_correct + df_confusion.loc[label, label]\n",
    "\n",
    "                # How many within errors?\n",
    "                # For expl:\n",
    "                expl_within = df_confusion.loc[l_label_order[:6], l_label_order[:6]].sum().sum() - correct_expl\n",
    "                # For cond:\n",
    "                cond_within = df_confusion.loc[l_label_order[6:], l_label_order[6:]].sum().sum() - correct_cond\n",
    "                # total:\n",
    "                total_within = expl_within + cond_within\n",
    "\n",
    "                # How many between errors?\n",
    "                # Predicted as expl but actually cond\n",
    "                expl_between = df_confusion.loc[l_label_order[6:], l_label_order[:6]].sum().sum()\n",
    "                # Predicted as cond but actually expl\n",
    "                cond_between = df_confusion.loc[l_label_order[:6], l_label_order[6:]].sum().sum()\n",
    "                # Total between errors:\n",
    "                total_between = expl_between + cond_between\n",
    "\n",
    "                # Sanity check: total errors + total correct must match test set label count:\n",
    "                total_errors = total_within + total_between\n",
    "                if total_correct + total_errors != df_confusion.sum().sum():\n",
    "                    print('Something went wrong for the error quantification of {} - run: {}'.format(approach, run))\n",
    "                    break\n",
    "                # Convert everything to percentage and append it to the lists in the temporary dictionary:\n",
    "                total_preds_expl = df_confusion[l_label_order[:6]].sum().sum()\n",
    "                total_preds_cond = df_confusion[l_label_order[6:]].sum().sum()\n",
    "                total_preds = total_preds_expl + total_preds_cond\n",
    "\n",
    "                d_temp['expl_true'].append(correct_expl / total_preds_expl * 100)\n",
    "                d_temp['expl_within'].append(expl_within / total_preds_expl * 100)\n",
    "                d_temp['expl_between'].append(expl_between / total_preds_expl * 100)\n",
    "\n",
    "                d_temp['cond_true'].append(correct_cond / total_preds_cond * 100)\n",
    "                d_temp['cond_within'].append(cond_within / total_preds_cond * 100)\n",
    "                d_temp['cond_between'].append(cond_between / total_preds_cond * 100)            \n",
    "\n",
    "                d_temp['total_true'].append(total_correct / total_preds * 100)\n",
    "                d_temp['total_within'].append(total_within / total_preds * 100)\n",
    "                d_temp['total_between'].append(total_between / total_preds * 100)              \n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if len(d_temp['expl_true']) > 1:\n",
    "            d_classification_results['Results']['Errors']['Individual'][approach] = d_temp\n",
    "\n",
    "\n",
    "            # If not already present, create the respective keys in the summary dictionary:\n",
    "            if len(d_classification_results['Results']['Errors']['Summary'].keys()) == 0:\n",
    "                d_classification_results['Results']['Errors']['Summary']['Approach'] = []\n",
    "                for key in d_temp.keys():\n",
    "                    d_classification_results['Results']['Errors']['Summary']['mean_{}'.format(key)] = []\n",
    "                    d_classification_results['Results']['Errors']['Summary']['mean_{}_stdev'.format(key)] = []\n",
    "\n",
    "            # Append the approach name as label and all mean values and stddevs:\n",
    "            d_classification_results['Results']['Errors']['Summary']['Approach'].append(approach)\n",
    "            for key in d_temp:\n",
    "                d_classification_results['Results']['Errors']['Summary']['mean_{}'.format(key)].append(mean(d_temp[key]))\n",
    "                d_classification_results['Results']['Errors']['Summary']['mean_{}_stdev'.format(key)].append(stdev(d_temp[key]))\n",
    "\n",
    "\n",
    "        #print('Done with {}'.format(approach))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b5724-0fde-4bbc-b51c-683451599000",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_pred_types = ['expl', 'cond', 'total']\n",
    "\n",
    "fig = plt.figure(figsize=(8, 25), facecolor='white')\n",
    "gs = fig.add_gridspec(3,1)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.9)\n",
    "\n",
    "for pred_type in l_pred_types:\n",
    "    l_labels = d_classification_results['Results']['Errors']['Summary']['Approach']\n",
    "    \n",
    "    l_means_correct = d_classification_results['Results']['Errors']['Summary']['mean_{}_true'.format(pred_type)]\n",
    "    l_means_correct_stdev = d_classification_results['Results']['Errors']['Summary']['mean_{}_true_stdev'.format(pred_type)]\n",
    "    \n",
    "    l_means_within = d_classification_results['Results']['Errors']['Summary']['mean_{}_within'.format(pred_type)]\n",
    "    l_means_within_stdev = d_classification_results['Results']['Errors']['Summary']['mean_{}_within_stdev'.format(pred_type)]  \n",
    "    \n",
    "    l_means_between = d_classification_results['Results']['Errors']['Summary']['mean_{}_between'.format(pred_type)]\n",
    "    l_means_between_stdev = d_classification_results['Results']['Errors']['Summary']['mean_{}_between_stdev'.format(pred_type)] \n",
    "    \n",
    "    l_sum_means_within_and_between = [x + y for (x, y) in zip(l_means_between, l_means_within)]\n",
    "    \n",
    "    # Bar width and label locations\n",
    "    width = 0.6\n",
    "    x = np.arange(len(l_labels))\n",
    "    \n",
    "    ax = fig.add_subplot(gs[l_pred_types.index(pred_type), 0])\n",
    "    \n",
    "    plt.bar(x, l_means_between, width, yerr=l_means_between_stdev, label='between error', color='red', edgecolor='black')\n",
    "    plt.bar(x, l_means_within, width, yerr=l_means_within_stdev, label='within error', color='orange', edgecolor='black', bottom=l_means_between)\n",
    "    plt.bar(x, l_means_correct, width, yerr=l_means_correct_stdev, label='correct', color='green', edgecolor='black', bottom=l_sum_means_within_and_between)\n",
    "    \n",
    "   \n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.xticks(x, l_labels, rotation='vertical')\n",
    "    plt.ylabel('Percentage of predictions')\n",
    "    #plt.xlabel('Approach')\n",
    "    plt.title(['Only predictions with label \"explorative session\"', 'Only predictions with label \"conditioning session\"', 'All predictions'][l_pred_types.index(pred_type)])\n",
    "    \n",
    "#plt.tight_layout()\n",
    "plt.savefig('Stacked_error_types.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d25da0-25e7-460c-a853-a18a62fd069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_and_evaluate_the_classifiers:\n",
    "    # Save the results:\n",
    "    with open('classifier_results_{}s_intervals.p'.format(str(interval_size_s)), 'wb') as fp:\n",
    "        pickle.dump(d_classification_results, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b2b33-4c6c-4e66-83f7-95e15bf862ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
