{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dangerous-reviewer",
   "metadata": {},
   "source": [
    "# __Quantify the clustering performance and the stability of cluster assignment__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-grammar",
   "metadata": {},
   "source": [
    "# Part 1) Compute UMAPs of continous or interval-binned data using the rapids-0.18 environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all dependencies from the rapids environment:\n",
    "\n",
    "from __future__ import print_function\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from cuml.manifold import UMAP as cuML_UMAP\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-quantum",
   "metadata": {},
   "source": [
    "## Load, clean, and shuffle the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CONT_DATA = True\n",
    "INTERVALLS_PER_DIMENSION = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'States_ceiling_reduced.csv'\n",
    "\n",
    "#filename = 'States_ceiling_mix2_with_UMAP.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-discovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_col_names = ['norm_HeartRate', \n",
    "               #'norm_HR_Low_Signal',\n",
    "               #'norm_HR_Med_Delta',\n",
    "               #'norm_HR_Med_Amp',\n",
    "               'norm_HR_High_Amp',\n",
    "               #'norm_HR_CoV_10s',\n",
    "               'norm_Ceiling',\n",
    "               'norm_DistanceToCeiling',\n",
    "               'norm_Motion',\n",
    "               'norm_AreaExplored',\n",
    "               'norm_Speed',\n",
    "               'norm_Temperature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "if filename == 'States_ceiling_reduced.csv':\n",
    "    # Since some adaptations had to be made \"on the fly\" to integrate the interval binned data, not sure whether it still works from scratch..\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Remove all datapoints that should be excluded:\n",
    "    df = df.loc[df['Exclude'] == False]\n",
    "\n",
    "    # Rename former index column that can be used later to match the different clusterings again after shuffling of the data:\n",
    "    l_colums = list(df.columns)\n",
    "    l_colums[0] = 'Data_idx'\n",
    "    df.columns = l_colums\n",
    "    \n",
    "    column_count = df.columns.shape[0]\n",
    "    # Should be re-usable to add other interval binnings as well and requires only updating of the column index:\n",
    "    for col_name in l_col_names:\n",
    "        n_dim_bins = INTERVALLS_PER_DIMENSION\n",
    "        new_col_name = col_name + '_intervals_' + str(n_dim_bins)\n",
    "        df.insert(column_count + l_col_names.index(col_name), new_col_name, pd.cut(df[col_name], n_dim_bins, labels=False, duplicates='drop') / n_dim_bins)\n",
    "\n",
    "else:\n",
    "    df = pd.read_csv(filename, index_col = 0)\n",
    "\n",
    "# Shuffle the data:\n",
    "df_shuffled = df.sample(frac=1).copy()\n",
    "\n",
    "# Show the DataFrame for visual inspection:\n",
    "df_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_dimensions = []\n",
    "\n",
    "if USE_CONT_DATA == False:\n",
    "    for col_name in l_col_names:\n",
    "        n_dim_bins = INTERVALLS_PER_DIMENSION\n",
    "        new_col_name = col_name + '_intervals_' + str(n_dim_bins)\n",
    "        df_shuffled[new_col_name] = pd.cut(df_shuffled[col_name], n_dim_bins, labels=False, duplicates='drop') / n_dim_bins\n",
    "        l_dimensions.append(new_col_name)\n",
    "else:\n",
    "    l_dimensions = l_col_names\n",
    "    \n",
    "print('\\nThese columns will be used as input for UMAP: \\n')\n",
    "print(l_dimensions)\n",
    "print('\\n \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-preview",
   "metadata": {},
   "source": [
    "For now, `n_neighbors` is set to 103, which is the result of the following computation if all valid datapoints are used: \n",
    "\n",
    "##### `Square-root of the number of all datapoints / (number of dimensions +1)`\n",
    "##### `int((df.shape[0] ** (1/2)) / len(l_dimensions)) +1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-entertainment",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = int((df_shuffled[l_dimensions].shape[0] ** (1/2)) / len(l_dimensions)) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_neighbors = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-routine",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_neighbors < 75:\n",
    "    n_neighbors = n_neighbors * 2\n",
    "    \n",
    "print(n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled[l_dimensions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-attempt",
   "metadata": {},
   "source": [
    "### Compute UMAP:\n",
    "\n",
    "Alright, let´s go. This step may need up to half an hour. To keep the time, starting time and finally the computation times will be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "print('UMAP computations were started at:')\n",
    "print(now.strftime(\"%d/%m/%Y %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "umap = cuML_UMAP(n_components=2, random_state = 42, n_neighbors=n_neighbors)\n",
    "umap_results = umap.fit_transform(df_shuffled[l_dimensions])\n",
    "print('UMAP done! Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-vertical",
   "metadata": {},
   "source": [
    "### Inspect the embedded space:\n",
    "\n",
    "Especially for the interval binned data the UMAP space is quite heavily spread out (yet only single points at these extremes). <br>\n",
    "To keep everything focused, both x- and y-axis are fixed between -25 and 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0.25\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10), facecolor='white')\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=umap_results[:,0],\n",
    "    y=umap_results[:,1],\n",
    "    s=size)\n",
    "plt.xlim(-25,25)\n",
    "plt.ylim(-25,25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-outreach",
   "metadata": {},
   "source": [
    "### Append the embedding coordinates to the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CONT_DATA:\n",
    "    embedding_col_name = 'UMAP_cont_r'\n",
    "else:\n",
    "    embedding_col_name = 'UMAP_intv_{}_r'.format(str(INTERVALLS_PER_DIMENSION))\n",
    "    \n",
    "df_shuffled[embedding_col_name + '1_1'] = umap_results[:, 0]\n",
    "df_shuffled[embedding_col_name + '1_2'] = umap_results[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled_results = df_shuffled[['Data_idx', embedding_col_name + '1_1', embedding_col_name + '1_2']].copy()\n",
    "\n",
    "df = pd.merge(df, df_shuffled_results, on='Data_idx', how='outer')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-novel",
   "metadata": {},
   "source": [
    "### If everything looks good so far, uncomment the following cell to save the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('States_ceiling_reduced3_with_UMAP.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-macro",
   "metadata": {},
   "source": [
    "### Since we now validated the workflow, let´s run the remaining `X` runs in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "print('Remaining computations were started at:')\n",
    "print(now.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "\n",
    "for run in range(X):\n",
    "    run = run + 2\n",
    "\n",
    "    # Re-do the shuffling for the new run:\n",
    "    df_shuffled = df.sample(frac=1).copy()\n",
    "\n",
    "    now = datetime.now()\n",
    "    print('UMAP computations of run {} were started at:'.format(run))\n",
    "    print(now.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "    \n",
    "    time_start = time.time()\n",
    "    umap = cuML_UMAP(n_components=2, random_state = 42, n_neighbors=n_neighbors)\n",
    "    umap_results = umap.fit_transform(df_shuffled[l_dimensions])\n",
    "    print('UMAP done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "    \n",
    "    df_shuffled[embedding_col_name + str(run) + '_1'] = umap_results[:, 0]\n",
    "    df_shuffled[embedding_col_name + str(run) + '_2'] = umap_results[:, 1]\n",
    "    \n",
    "    df_shuffled_results = df_shuffled[['Data_idx', embedding_col_name + str(run) + '_1', embedding_col_name + str(run) + '_2']].copy()\n",
    "\n",
    "    df = pd.merge(df, df_shuffled_results, on='Data_idx', how='outer')\n",
    "    \n",
    "    print('Done with run {}! :)'.format(run))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-feelings",
   "metadata": {},
   "source": [
    "### Now let´s compare the three UMAP \"state spaces\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1\n",
    "\n",
    "column_specifier = 'cont'\n",
    "#column_specifier = 'intv_100'\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10), facecolor='white')\n",
    "gs = fig.add_gridspec(1,3)\n",
    "\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x='UMAP_{}_r1_1'.format(column_specifier),\n",
    "    y='UMAP_{}_r1_2'.format(column_specifier),\n",
    "    s=size,\n",
    "    ax=ax1)\n",
    "plt.title('Run 1')\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0,1])\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x='UMAP_{}_r2_1'.format(column_specifier),\n",
    "    y='UMAP_{}_r2_2'.format(column_specifier),\n",
    "    s=size,\n",
    "    ax=ax2)\n",
    "plt.title('Run 2')\n",
    "\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0,2])\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x='UMAP_{}_r3_1'.format(column_specifier),\n",
    "    y='UMAP_{}_r3_2'.format(column_specifier),\n",
    "    s=size,\n",
    "    ax=ax3)\n",
    "plt.title('Run 3')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-matthew",
   "metadata": {},
   "source": [
    "### All good? Then save the results and either start again to compute UMAPs of interval binned data or continue with the clustering of the data! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('States_ceiling_reduced3_with_UMAP.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-polymer",
   "metadata": {},
   "source": [
    "# Part 2) Cluster the embedded spaces using the HDBSCAN environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all dependencies from the HDBSCAN environment:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Until the joblib bug is fixed (https://github.com/scikit-learn-contrib/hdbscan/issues/436) - joblib version 0.17 (instead of latest: 1.0.0) has to be used!\n",
    "import joblib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-diana",
   "metadata": {},
   "source": [
    "### Confirm that joblib version 0.17 is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-creativity",
   "metadata": {},
   "source": [
    "### Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('States_ceiling_reduced3_with_UMAP.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-inquiry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-pharmacy",
   "metadata": {},
   "source": [
    "### Before we even start with the clustering, let´s define the parameter `n_neighbors`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Exclude'] == False].groupby('behaviors').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-island",
   "metadata": {},
   "source": [
    "#### min_cluster_size is set to 161, which reflects 25% of the bin_count of the least frequently occuring behavior, calculated like:\n",
    "###### `int(df.loc[df['behaviors'].isin(['Flight', 'Rearing', 'StretchAttend', 'TailRattling', 'Grooming', 'Immobility', 'HeadDips'])].groupby('behaviors').count().min().min()*0.25)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size = 161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for code in ['cont', 'intv_100']:\n",
    "for code in ['cont']:\n",
    "    for run in range(1, 4):\n",
    "        l_features = ['UMAP_{}_r{}_1'.format(code, str(run)), 'UMAP_{}_r{}_2'.format(code, str(run))]\n",
    "        df_to_cluster = df[l_features].copy()\n",
    "        \n",
    "        cluster_column = 'Cluster_{}_r{}'.format(code, str(run))\n",
    "        hdbscan = HDBSCAN(min_cluster_size=min_cluster_size, cluster_selection_method='leaf', allow_single_cluster=True).fit(df_to_cluster[l_features])\n",
    "        df[cluster_column] = hdbscan.labels_\n",
    "    df.to_csv('Clustered_nN-{}_States_ceiling_reduced3_with_UMAP.csv'.format(min_cluster_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-lewis",
   "metadata": {},
   "source": [
    "#### Set the variables to inspect the clustering. You can also highlight the cluster IDs of a different run to visually inspect the consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_features = 'UMAP_cont_r1_1', 'UMAP_cont_r1_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_column = 'Cluster_cont_r1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-terry",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 14))\n",
    "sns.scatterplot(\n",
    "    x=l_features[0],\n",
    "    y=l_features[1],\n",
    "    hue=cluster_column,\n",
    "    palette='Spectral',\n",
    "    data=df,\n",
    "    legend=False,\n",
    "    #alpha=0.3,\n",
    "    s=1)\n",
    "plt.ylim(-25, 25)\n",
    "plt.xlim(-25, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 15), facecolor='white')\n",
    "\n",
    "gs = fig.add_gridspec(2,4)\n",
    "\n",
    "fig.add_subplot(gs[0,0])\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=l_features[0],\n",
    "    y=l_features[1],\n",
    "    #hue='Session',\n",
    "    #palette='colorblind',\n",
    "    color = 'goldenrod',\n",
    "    data=df.loc[df['Session'] == 'OF'],\n",
    "    legend=False,\n",
    "    #alpha=0.3,\n",
    "    s=0.5)\n",
    "plt.ylim(-15, 15)\n",
    "plt.xlim(-15, 15)\n",
    "plt.title('OF')\n",
    "\n",
    "fig.add_subplot(gs[0,1])\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=l_features[0],\n",
    "    y=l_features[1],\n",
    "    #hue='Session',\n",
    "    #palette='colorblind',\n",
    "    color = 'forestgreen',\n",
    "    data=df.loc[df['Session'] == 'EPM'],\n",
    "    legend=False,\n",
    "    #alpha=0.3,\n",
    "    s=0.5)\n",
    "plt.ylim(-15, 15)\n",
    "plt.xlim(-15, 15)\n",
    "plt.title('EPM')\n",
    "\n",
    "fig.add_subplot(gs[1,0])\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=l_features[0],\n",
    "    y=l_features[1],\n",
    "    #hue='Session',\n",
    "    #palette='colorblind',\n",
    "    color='dodgerblue',\n",
    "    data=df.loc[df['Session'] == 'CD1'],\n",
    "    legend=False,\n",
    "    #alpha=0.3,\n",
    "    s=0.5)\n",
    "plt.ylim(-15, 15)\n",
    "plt.xlim(-15, 15)\n",
    "plt.title('CD1')\n",
    "\n",
    "fig.add_subplot(gs[1,1])\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=l_features[0],\n",
    "    y=l_features[1],\n",
    "    #hue='Session',\n",
    "    #palette='colorblind',\n",
    "    color = 'magenta',\n",
    "    data=df.loc[df['Session'] == 'CD2'],\n",
    "    legend=False,\n",
    "    #alpha=0.3,\n",
    "    s=0.5)\n",
    "plt.ylim(-15, 15)\n",
    "plt.xlim(-15, 15)\n",
    "plt.title('CD2')\n",
    "\n",
    "\n",
    "fig.add_subplot(gs[0:2,2:4])\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=l_features[0],\n",
    "    y=l_features[1],\n",
    "    hue='Session',\n",
    "    palette=['goldenrod', 'forestgreen', 'dodgerblue', 'magenta'],\n",
    "    #color = 'forestgreen',\n",
    "    data=df,\n",
    "    legend=True,\n",
    "    alpha=0.3,\n",
    "    s=2)\n",
    "plt.ylim(-15, 15)\n",
    "plt.xlim(-15, 15)\n",
    "\n",
    "#plt.savefig('State_spaces_reduced2.png', dpi=600)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-vessel",
   "metadata": {},
   "source": [
    "# Part 3) Finally, let´s compare the clustering performance and the consistency of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all dependencies of the base environment:\n",
    "from __future__ import print_function\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-canal",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Clustered_nN-161_States_ceiling_reduced3_with_UMAP.csv'\n",
    "\n",
    "#filename = 'Clustered_nN-161_States_ceiling_reduced_with_UMAP_with_similarity_results.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename, index_col = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-heather",
   "metadata": {},
   "source": [
    "### This first section is intended to quantify the general clustering performance, using metrics that are implemented in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results_no_noise = {}\n",
    "#for code in ['cont', 'intv_100']:\n",
    "for code in ['cont']:\n",
    "    results[code] = {}\n",
    "    results_no_noise[code] = {}\n",
    "    for run in range(1, 4):\n",
    "        l_features = ['UMAP_{}_r{}_1'.format(code, str(run)), 'UMAP_{}_r{}_2'.format(code, str(run))]\n",
    "        cluster_column = 'Cluster_{}_r{}'.format(code, str(run))\n",
    "        \n",
    "        d_b_score = metrics.davies_bouldin_score(df[l_features], df[cluster_column])\n",
    "        c_h_score = metrics.calinski_harabasz_score(df[l_features], df[cluster_column])\n",
    "        \n",
    "        results[code]['run_{}'.format(str(run))] = [d_b_score, c_h_score]\n",
    "        \n",
    "        df_temp = df.loc[df[cluster_column] != -1].copy()\n",
    "        d_b_score_no_noise = metrics.davies_bouldin_score(df_temp[l_features], df_temp[cluster_column])\n",
    "        c_h_score_no_noise = metrics.calinski_harabasz_score(df_temp[l_features], df_temp[cluster_column])\n",
    "        \n",
    "        results_no_noise[code]['run_{}'.format(str(run))] = [d_b_score, c_h_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-cooling",
   "metadata": {},
   "source": [
    "Clustering results from UMAPS with continous values seems to be more stable, compared to larger fluctuations after intervall binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-colon",
   "metadata": {},
   "source": [
    "Clustering performance measures are not affected by those datapoints that are classified as noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_no_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-fantasy",
   "metadata": {},
   "source": [
    "### This section is based on custom written code to quantify the similarity between the cluster results of two runs\n",
    "#### These similarities should provide a measure for stability of the detected clusters across runs. Be aware that these computations take ~ 1.5 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "Execute_similarity_computations = True\n",
    "filename_suffix = 'ceiling_reduced3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Execute_similarity_computations:\n",
    "    print(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "\n",
    "    time_start = time.time()\n",
    "    similarity_results = {}\n",
    "\n",
    "    # for cont & intv\n",
    "    code = 'cont'\n",
    "\n",
    "    similarity_results[code] = {}\n",
    "\n",
    "    # for all possible run combinations:\n",
    "    for run_combo in [('1', '2'), ('1', '3'), ('2', '1'), ('2', '3'), ('3', '1'), ('3', '2')]:\n",
    "    #for run_combo in [('1', '2')]:\n",
    "    #for run_combo in [('1', '3'), ('2', '1'), ('2', '3'), ('3', '1'), ('3', '2')]:\n",
    "\n",
    "        run_a, run_b = run_combo[0], run_combo[1]\n",
    "\n",
    "        similarity_results[code]['run_{}_vs_{}'.format(run_a, run_b)] = {}\n",
    "\n",
    "        cluster_col_a = 'Cluster_{}_r{}'.format(code, run_a)\n",
    "        cluster_col_b = 'Cluster_{}_r{}'.format(code, run_b)\n",
    "\n",
    "        # If this is written to automatically iterate through all clusterings, remember to add the proper data selection from the main df:\n",
    "        df_to_check = df[['Data_idx', cluster_col_a, cluster_col_b]].copy()\n",
    "\n",
    "        for Data_idx in df_to_check['Data_idx'].unique():\n",
    "            cluster_id_a = df_to_check.loc[df_to_check['Data_idx'] == Data_idx, cluster_col_a].values[0]\n",
    "            if cluster_id_a != -1:\n",
    "                # Check if this cluster has already its own key and create one, if not:\n",
    "                if cluster_id_a not in similarity_results[code]['run_{}_vs_{}'.format(run_a, run_b)].keys():\n",
    "                    similarity_results[code]['run_{}_vs_{}'.format(run_a, run_b)][cluster_id_a] = {'Matching_cluster_IDs': [],\n",
    "                                                                                                   'Similarities_in_perc': []}\n",
    "                # Retrieve the cluster id to which Data_idx was assigned to in run_b:\n",
    "                cluster_id_b = df_to_check.loc[df_to_check['Data_idx'] == Data_idx, cluster_col_b].values[0]\n",
    "\n",
    "                # Check whether it belongs to a real cluster and not to noise:\n",
    "                if cluster_id_b != -1:\n",
    "\n",
    "                    # Okay, the datapoint was assigned to a cluster in both runs. Let´s check for the similarity (if it was not already computed):\n",
    "                    if cluster_id_b not in similarity_results[code]['run_{}_vs_{}'.format(run_a, run_b)][cluster_id_a]['Matching_cluster_IDs']:\n",
    "\n",
    "                        # The similarities between these two clusters has not been computed yet, so let´s do it:\n",
    "                        cluster_id_a_members = df_to_check.loc[df_to_check[cluster_col_a] == cluster_id_a, 'Data_idx'].values\n",
    "                        cluster_id_b_members = df_to_check.loc[df_to_check[cluster_col_b] == cluster_id_b, 'Data_idx'].values\n",
    "\n",
    "                        points_in_both_clusters = np.intersect1d(cluster_id_a_members, cluster_id_b_members)\n",
    "                        similarity_a_to_b = points_in_both_clusters.shape[0] / cluster_id_a_members.shape[0] * 100\n",
    "\n",
    "                        # Append the results to the dictionary:\n",
    "                        similarity_results[code]['run_{}_vs_{}'.format(run_a, run_b)][cluster_id_a]['Matching_cluster_IDs'].append(cluster_id_b)\n",
    "                        similarity_results[code]['run_{}_vs_{}'.format(run_a, run_b)][cluster_id_a]['Similarities_in_perc'].append((cluster_id_b, similarity_a_to_b))\n",
    "\n",
    "                    else:\n",
    "                        # The similarity between these two clusters has already been calculated. So we can simply retrieve the result:\n",
    "                        similarity_a_to_b = [item for item in similarity_results[code]['run_{}_vs_{}'.format(run_a, run_b)][cluster_id_a]['Similarities_in_perc'] if item[0] == cluster_id_b][0][1]\n",
    "\n",
    "                    sim_suffix = str(int(round(similarity_a_to_b, 0))).zfill(3)\n",
    "                    classification = 'ac_bc_{}'.format(sim_suffix)\n",
    "\n",
    "\n",
    "\n",
    "                else: # Specify error type. Here the datapoint was in a cluster in a but was noise in b\n",
    "                    classification = 'ac_bn'\n",
    "\n",
    "            else: # Check for potential errors. Is the datapoint also assigned to noise or into a cluster in b? \n",
    "                cluster_id_b = df_to_check.loc[df_to_check['Data_idx'] == Data_idx, cluster_col_b].values[0]\n",
    "\n",
    "                if cluster_id_b != -1:\n",
    "                    classification = 'an_bc'\n",
    "\n",
    "                else:\n",
    "                    classification = 'an_bn'\n",
    "\n",
    "            df_to_check.loc[df_to_check['Data_idx'] == Data_idx, 'Comparison_{}_run_{}_vs_{}'.format(code, run_a, run_b)] = classification \n",
    "\n",
    "            # Because these computations take quite some time, let´s print from time to time how it´s progressing:\n",
    "            if df_to_check.loc[df_to_check['Data_idx'] == Data_idx].index[0] % 50000 == 0:\n",
    "\n",
    "                print('This is run {} vs run {}'.format(run_a, run_b))\n",
    "                print(list(df_to_check['Data_idx'].unique()).index(Data_idx))    \n",
    "                print('Time passed so far:')\n",
    "                print(time.time()-time_start)\n",
    "                print('######################')\n",
    "\n",
    "        # Save the results:\n",
    "        with open('similarity_results_{}_r{}_vs_r{}_{}.p'.format(code, run_a, run_b, filename_suffix), 'wb') as fp:\n",
    "            pickle.dump(similarity_results, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # To automatize the merging into the main df, check whether this actually works. Otherwise, use the cells below\n",
    "        if df.shape[0] == df_to_check.shape[0]:\n",
    "            df = pd.merge(df, df_to_check[['Data_idx', 'Comparison_{}_run_{}_vs_{}'.format(code, run_a, run_b)]], on='Data_idx', how='outer')\n",
    "            # As safety net, let´s save the data in another .csv:\n",
    "            df.to_csv('Clustered_nN-161_States_ceiling_reduced3_with_UMAP_with_similarity_results.csv')\n",
    "            print('All data was successfully saved!')\n",
    "        else:\n",
    "            print('There was an issue with saving the DataFrame!')\n",
    "\n",
    "else:\n",
    "    print('If you want to execute this code, please remember to set `Execute_similarity_computations` to True')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-suggestion",
   "metadata": {},
   "source": [
    "##### Inspect, whether the column with the classifications of each data point was correctly added to the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-academy",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Get some rough overview on the cluster similarities, subdivided into pairs of high (66-100%), medium (33-66%), and low (0-33%) similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-chemical",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "no_cluster_similarity = {}\n",
    "low_cluster_similarity = {}\n",
    "medium_cluster_similarity = {}\n",
    "high_cluster_similarity = {}\n",
    "\n",
    "for cluster_id in similarity_results[code]['run_{}_vs_{}'.format(run_a, run_b)].keys():\n",
    "    l_high = [item for item in similarity_results[code]['run_{}_vs_{}'.format(run_a, run_b)][cluster_id]['Similarities_in_perc'] if item[1] > 66]\n",
    "    l_medium = [item for item in similarity_results[code]['run_{}_vs_{}'.format(run_a, run_b)][cluster_id]['Similarities_in_perc'] if (item[1] > 33) & (item[1] <= 66)]\n",
    "    l_low = [item for item in similarity_results[code]['run_{}_vs_{}'.format(run_a, run_b)][cluster_id]['Similarities_in_perc'] if (item[1] > 0) & (item[1] <= 33)]\n",
    "    l_none = [item for item in similarity_results[code]['run_{}_vs_{}'.format(run_a, run_b)][cluster_id]['Similarities_in_perc'] if item[1]==0]\n",
    "    if len(l_high) > 0:\n",
    "        high_cluster_similarity[cluster_id] = l_high\n",
    "    \n",
    "    if len(l_medium) > 0:\n",
    "        medium_cluster_similarity[cluster_id] = l_medium\n",
    "    \n",
    "    if len(l_low) > 0:\n",
    "        low_cluster_similarity[cluster_id] = l_low\n",
    "    \n",
    "    if len(l_none) > 0:\n",
    "        no_cluster_similarity[cluster_id] = l_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-destination",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "high_cluster_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-contamination",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "medium_cluster_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-webmaster",
   "metadata": {},
   "source": [
    "### Let´s see, if we can identify clusters that are found with a certain similarity threshold across all three runs\n",
    "\n",
    "Ideas how this could be extended:\n",
    "* If we would use soft clustering, we could check wether those datapoints that are assigned to a cluster in run a but not in run b, <br> have at least a high probability of belonging to the respective matching cluster in b?\n",
    "* Sticking to the \"hard\" clustering, we could also check how much of the remaining points (not assigned to the matching cluster) are assigned to a different cluster (potentially problematic if this cluster is not matching), or whether they are classified as noise (low distance to cluster border)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = 'cont'\n",
    "run_a = '1'\n",
    "run_b = '2'\n",
    "run_c = '3'\n",
    "\n",
    "similarity_threshold = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the similarity results:\n",
    "with open('similarity_results_{}_r{}_vs_r{}_{}.p'.format(code, run_a, run_b, filename_suffix), 'rb') as fp:\n",
    "    similarity_results_a_vs_b = pickle.load(fp)\n",
    "    \n",
    "with open('similarity_results_{}_r{}_vs_r{}_{}.p'.format(code, run_b, run_a, filename_suffix), 'rb') as fp:\n",
    "    similarity_results_b_vs_a = pickle.load(fp)\n",
    "\n",
    "with open('similarity_results_{}_r{}_vs_r{}_{}.p'.format(code, run_a, run_c, filename_suffix), 'rb') as fp:\n",
    "    similarity_results_a_vs_c = pickle.load(fp)\n",
    "    \n",
    "with open('similarity_results_{}_r{}_vs_r{}_{}.p'.format(code, run_c, run_a, filename_suffix), 'rb') as fp:\n",
    "    similarity_results_c_vs_a = pickle.load(fp)\n",
    "    \n",
    "with open('similarity_results_{}_r{}_vs_r{}_{}.p'.format(code, run_b, run_c, filename_suffix), 'rb') as fp:\n",
    "    similarity_results_b_vs_c = pickle.load(fp)\n",
    "    \n",
    "with open('similarity_results_{}_r{}_vs_r{}_{}.p'.format(code, run_c, run_b, filename_suffix), 'rb') as fp:\n",
    "    similarity_results_c_vs_b = pickle.load(fp)\n",
    "\n",
    "# For each combination of two runs, find all matching cluster pairs from these two runs that exceed the similarity_threshold (both directions will be checked):\n",
    "\n",
    "# For run_a & run_b:\n",
    "matching_clusters_a_b = {'run_{}_vs_{}'.format(run_a, run_b): {},\n",
    "                         'run_{}_vs_{}'.format(run_b, run_a): {}}\n",
    "\n",
    "for cluster_id in similarity_results_a_vs_b[code]['run_{}_vs_{}'.format(run_a, run_b)].keys():\n",
    "    l_matching_clusters_a_b = [item for item in similarity_results_a_vs_b[code]['run_{}_vs_{}'.format(run_a, run_b)][cluster_id]['Similarities_in_perc'] if item[1] >= similarity_threshold]\n",
    "    if len(l_matching_clusters_a_b) > 0:\n",
    "        matching_clusters_a_b['run_{}_vs_{}'.format(run_a, run_b)][cluster_id] = l_matching_clusters_a_b\n",
    "        \n",
    "for cluster_id in similarity_results_b_vs_a[code]['run_{}_vs_{}'.format(run_b, run_a)].keys():\n",
    "    l_matching_clusters_b_a = [item for item in similarity_results_b_vs_a[code]['run_{}_vs_{}'.format(run_b, run_a)][cluster_id]['Similarities_in_perc'] if item[1] >= similarity_threshold]\n",
    "    if len(l_matching_clusters_b_a) > 0:\n",
    "        matching_clusters_a_b['run_{}_vs_{}'.format(run_b, run_a)][cluster_id] = l_matching_clusters_b_a\n",
    "        \n",
    "\n",
    "# For run_a & run_c:\n",
    "matching_clusters_a_c = {'run_{}_vs_{}'.format(run_a, run_c): {},\n",
    "                         'run_{}_vs_{}'.format(run_c, run_a): {}}\n",
    "\n",
    "for cluster_id in similarity_results_a_vs_c[code]['run_{}_vs_{}'.format(run_a, run_c)].keys():\n",
    "    l_matching_clusters_a_c = [item for item in similarity_results_a_vs_c[code]['run_{}_vs_{}'.format(run_a, run_c)][cluster_id]['Similarities_in_perc'] if item[1] >= similarity_threshold]\n",
    "    if len(l_matching_clusters_a_c) > 0:\n",
    "        matching_clusters_a_c['run_{}_vs_{}'.format(run_a, run_c)][cluster_id] = l_matching_clusters_a_c\n",
    "        \n",
    "for cluster_id in similarity_results_c_vs_a[code]['run_{}_vs_{}'.format(run_c, run_a)].keys():\n",
    "    l_matching_clusters_c_a = [item for item in similarity_results_c_vs_a[code]['run_{}_vs_{}'.format(run_c, run_a)][cluster_id]['Similarities_in_perc'] if item[1] >= similarity_threshold]\n",
    "    if len(l_matching_clusters_c_a) > 0:\n",
    "        matching_clusters_a_c['run_{}_vs_{}'.format(run_c, run_a)][cluster_id] = l_matching_clusters_c_a\n",
    "        \n",
    "\n",
    "# For run_b & run_c:\n",
    "matching_clusters_b_c = {'run_{}_vs_{}'.format(run_b, run_c): {},\n",
    "                         'run_{}_vs_{}'.format(run_c, run_b): {}}\n",
    "\n",
    "for cluster_id in similarity_results_b_vs_c[code]['run_{}_vs_{}'.format(run_b, run_c)].keys():\n",
    "    l_matching_clusters_b_c = [item for item in similarity_results_b_vs_c[code]['run_{}_vs_{}'.format(run_b, run_c)][cluster_id]['Similarities_in_perc'] if item[1] >= similarity_threshold]\n",
    "    if len(l_matching_clusters_b_c) > 0:\n",
    "        matching_clusters_b_c['run_{}_vs_{}'.format(run_b, run_c)][cluster_id] = l_matching_clusters_b_c\n",
    "        \n",
    "for cluster_id in similarity_results_c_vs_b[code]['run_{}_vs_{}'.format(run_c, run_b)].keys():\n",
    "    l_matching_clusters_c_b = [item for item in similarity_results_c_vs_b[code]['run_{}_vs_{}'.format(run_c, run_b)][cluster_id]['Similarities_in_perc'] if item[1] >= similarity_threshold]\n",
    "    if len(l_matching_clusters_c_b) > 0:\n",
    "        matching_clusters_b_c['run_{}_vs_{}'.format(run_c, run_b)][cluster_id] = l_matching_clusters_c_b\n",
    "\n",
    "\n",
    "# For each combination of two runs, we now have the smilarity-results for both directions. \n",
    "# Let´s go through these results to find all unique combinations (in this step, we lose the inforamtion about the directionality, which doesn´t really matter after all)\n",
    "\n",
    "# For run_a & run_b:\n",
    "l_matching_pairs_ab = []\n",
    "\n",
    "for cluster_id_a in matching_clusters_a_b[list(matching_clusters_a_b.keys())[0]].keys():\n",
    "    l_matches = matching_clusters_a_b[list(matching_clusters_a_b.keys())[0]][cluster_id_a]\n",
    "    for i in range(len(l_matches)):\n",
    "        l_matching_pairs_ab.append((cluster_id_a, matching_clusters_a_b[list(matching_clusters_a_b.keys())[0]][cluster_id_a][i][0]))\n",
    "\n",
    "for cluster_id_b in matching_clusters_a_b[list(matching_clusters_a_b.keys())[1]].keys():\n",
    "    l_matches = matching_clusters_a_b[list(matching_clusters_a_b.keys())[1]][cluster_id_b]\n",
    "    for i in range(len(l_matches)):\n",
    "        l_matching_pairs_ab.append((matching_clusters_a_b[list(matching_clusters_a_b.keys())[1]][cluster_id_b][i][0], cluster_id_b))\n",
    "        \n",
    "l_matching_pairs_ab = list(set(l_matching_pairs_ab))\n",
    "\n",
    "\n",
    "# For run_a & run_b:\n",
    "l_matching_pairs_ac = []\n",
    "\n",
    "for cluster_id_a in matching_clusters_a_c[list(matching_clusters_a_c.keys())[0]].keys():\n",
    "    l_matches = matching_clusters_a_c[list(matching_clusters_a_c.keys())[0]][cluster_id_a]\n",
    "    for i in range(len(l_matches)):\n",
    "        l_matching_pairs_ac.append((cluster_id_a, matching_clusters_a_c[list(matching_clusters_a_c.keys())[0]][cluster_id_a][i][0]))\n",
    "        \n",
    "for cluster_id_c in matching_clusters_a_c[list(matching_clusters_a_c.keys())[1]].keys():\n",
    "    l_matches = matching_clusters_a_c[list(matching_clusters_a_c.keys())[1]][cluster_id_c]\n",
    "    for i in range(len(l_matches)):\n",
    "        l_matching_pairs_ac.append((matching_clusters_a_c[list(matching_clusters_a_c.keys())[1]][cluster_id_c][i][0], cluster_id_c))\n",
    "        \n",
    "l_matching_pairs_ac = list(set(l_matching_pairs_ac))\n",
    "\n",
    "\n",
    "# For run_b & run_c:\n",
    "l_matching_pairs_bc = []\n",
    "\n",
    "for cluster_id_b in matching_clusters_b_c[list(matching_clusters_b_c.keys())[0]].keys():\n",
    "    l_matches = matching_clusters_b_c[list(matching_clusters_b_c.keys())[0]][cluster_id_b]\n",
    "    for i in range(len(l_matches)):\n",
    "        l_matching_pairs_bc.append((cluster_id_b, matching_clusters_b_c[list(matching_clusters_b_c.keys())[0]][cluster_id_b][i][0]))\n",
    "        \n",
    "for cluster_id_c in matching_clusters_b_c[list(matching_clusters_b_c.keys())[1]].keys():\n",
    "    l_matches = matching_clusters_b_c[list(matching_clusters_b_c.keys())[1]][cluster_id_c]\n",
    "    for i in range(len(l_matches)):\n",
    "        l_matching_pairs_bc.append((matching_clusters_b_c[list(matching_clusters_b_c.keys())[1]][cluster_id_c][i][0], cluster_id_c))\n",
    "        \n",
    "l_matching_pairs_bc = list(set(l_matching_pairs_bc))\n",
    "\n",
    "\n",
    "# Now that we have all unique matching cluster pairs for all run combinations, let´s try to find those clusters that have a matching cluster in all three runs:\n",
    "l_matching_triplets_abc = []\n",
    "for pair_id in range(len(l_matching_pairs_ab)):\n",
    "    cluster_id_a, cluster_id_b = l_matching_pairs_ab[pair_id][0], l_matching_pairs_ab[pair_id][1]\n",
    "    \n",
    "    l_matching_cluster_ids_c_from_a = []\n",
    "    for i in range(len(l_matching_pairs_ac)):\n",
    "        if l_matching_pairs_ac[i][0] == cluster_id_a:\n",
    "            l_matching_cluster_ids_c_from_a.append(l_matching_pairs_ac[i][1])\n",
    "            \n",
    "    l_matching_cluster_ids_c_from_b = []\n",
    "    for i in range(len(l_matching_pairs_bc)):\n",
    "        if l_matching_pairs_bc[i][0] == cluster_id_b:\n",
    "            l_matching_cluster_ids_c_from_b.append(l_matching_pairs_bc[i][1])   \n",
    "    \n",
    "    l_cluster_ids_c = []\n",
    "    for cluster_id_c_from_a in l_matching_cluster_ids_c_from_a:\n",
    "        if cluster_id_c_from_a in l_matching_cluster_ids_c_from_b:\n",
    "            l_cluster_ids_c.append(cluster_id_c_from_a)\n",
    "    \n",
    "    for cluster_id_c in l_cluster_ids_c:\n",
    "        l_matching_triplets_abc.append([(run_a, cluster_id_a) , (run_b, cluster_id_b), (run_c, cluster_id_c)])\n",
    "        \n",
    "if len(l_matching_triplets_abc) > 0:\n",
    "    print('Congratulations! You found {} matching cluster tripletts across all runs! :-)'.format(len(l_matching_triplets_abc)))\n",
    "    \n",
    "else: \n",
    "    print('I´m sorry. I could not find any cluster tripletts that fullfill your criteria! :-(')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-present",
   "metadata": {},
   "source": [
    "### Now let´s plot all matching cluster triplets to inspect how the individual measurements look like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_measure_columns = {'intv_100': [column for column in df.columns if column.endswith('_intervals_100')],\n",
    "                     'cont': [column[:column.index('_intervals_100')] for column in df.columns if column.endswith('_intervals_100')]}\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,5*len(l_matching_triplets_abc)), facecolor='white')\n",
    "\n",
    "gs = fig.add_gridspec(len(l_matching_triplets_abc), 1)\n",
    "\n",
    "for row in range(len(l_matching_triplets_abc)):\n",
    "    l_dfs_individual_clusters = []\n",
    "    for elem in l_matching_triplets_abc[row]:\n",
    "        run, cluster_id = elem\n",
    "        l_dfs_single_measurements = []\n",
    "        for measure in d_measure_columns[code]:\n",
    "            df_temp = df.loc[df['Cluster_{}_r{}'.format(code, run)] == cluster_id, ['Data_idx', 'Animal_ID', measure]].copy()\n",
    "            df_temp.columns = ['Data_idx', 'Animal_ID', 'Data']\n",
    "            n_datapoints = df_temp.shape[0]\n",
    "            N_mice = df_temp['Animal_ID'].unique().shape[0]\n",
    "            df_temp['Measure'] = measure\n",
    "            df_temp['Cluster_Run'] = 'Cluster-{}_Run-{} ({} datapoints from {} mice)'.format(str(cluster_id), run, n_datapoints, N_mice)\n",
    "            l_dfs_single_measurements.append(df_temp)\n",
    "        l_dfs_individual_clusters.append(pd.concat(l_dfs_single_measurements))\n",
    "\n",
    "    df_for_boxplots = pd.concat(l_dfs_individual_clusters)\n",
    "    \n",
    "    fig.add_subplot(gs[row, 0])\n",
    "    sns.boxplot(x=\"Measure\", y=\"Data\", hue=\"Cluster_Run\", data=df_for_boxplots)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-globe",
   "metadata": {},
   "source": [
    "## Plot these data back onto the original traces. How does it look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-preview",
   "metadata": {},
   "source": [
    "## Plotting only those bins that belong to the stable clusters and were found exclusively in run 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_mice = df['Animal_ID'].unique()\n",
    "#l_mice = [df['Animal_ID'].unique()[3]]\n",
    "#l_mice = ['175_F4-19']\n",
    "\n",
    "\n",
    "folder = 'Plots_states_over_raw_data_all_mice_ceiling_reduced3'\n",
    "\n",
    "SAVE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_matching_triplets_r1 = [elem[0][1] for elem in l_matching_triplets_abc]\n",
    "\n",
    "\n",
    "for mouse in l_mice:\n",
    "    l_sessions = df.loc[df['Animal_ID'] == mouse, 'Session'].unique()\n",
    "    #l_sessions = ['CD2']\n",
    "    \n",
    "    for session in l_sessions:\n",
    "        # Select the corresponding session data\n",
    "        df_session = df.loc[(df['Animal_ID'] == mouse) & (df['Session'] == session)].copy()\n",
    "        \n",
    "        # Since df was entirely cleaned from NaNs already, we have to recreate the gaps so that they appear in the plotted traces and are not connected\n",
    "        last_bin = df_session['Bin'].max()\n",
    "        data = {'Bin': [int(elem) for elem in np.linspace(1, last_bin, last_bin)]}\n",
    "        df_bins = pd.DataFrame (data=data)\n",
    "        df_merged = pd.merge(df_session, df_bins, on='Bin', how='right')\n",
    "        \n",
    "        # To show only relevant behaviors and relevant states, we will create corresponding columns that can be used to specify the hue\n",
    "        df_merged['matching_cluster_ids'] = np.NaN\n",
    "        df_merged.loc[df_merged['Cluster_cont_r1'].isin(l_matching_triplets_r1), 'matching_cluster_ids'] = df_merged['Cluster_cont_r1']\n",
    "\n",
    "        l_selected_behaviors = ['Immobility', 'Grooming', 'Rearing', 'Flight', 'StretchAttend', 'TailRattling', 'HeadDips']\n",
    "        df_merged['selected_behaviors'] = np.NaN\n",
    "        df_merged.loc[df_merged['behaviors'].isin(l_selected_behaviors), 'selected_behaviors'] = df_merged['behaviors']\n",
    "        \n",
    "        # Also, we need some fixed y-value for the scatterplots of behaviors and states:\n",
    "        df_merged['plain_one'] = 1\n",
    "        \n",
    "        # Now we are ready to create the figure: \n",
    "        fig = plt.figure(figsize=(20, 8), facecolor='white')\n",
    "        gs = fig.add_gridspec(14,1)\n",
    "\n",
    "        ax1 = fig.add_subplot(gs[0,0])\n",
    "        sns.scatterplot(data=df_merged, x='Bin', y='plain_one', hue='selected_behaviors', marker=\"|\", palette='colorblind', legend=False, s=100, ax=ax1)\n",
    "        plt.title('{} during {}'.format(mouse, session))\n",
    "        plt.xlim(0,last_bin)\n",
    "        plt.ylabel('behavior', rotation=0)\n",
    "        plt.xlabel('')\n",
    "        ax1.yaxis.set_label_coords(-0.05,0.3)\n",
    "\n",
    "        ax2 = fig.add_subplot(gs[1, 0], sharex=ax1)\n",
    "        sns.scatterplot(data=df_merged, x='Bin', y='plain_one', hue='matching_cluster_ids', marker=\"|\", palette='Spectral', legend=False, s=100, ax=ax2)\n",
    "        plt.ylabel('state', rotation=0)\n",
    "        plt.xlabel('')\n",
    "        ax2.yaxis.set_label_coords(-0.05,0.3)\n",
    "\n",
    "        ax3 = fig.add_subplot(gs[2:6, 0], sharex=ax1)\n",
    "        plt.plot(df_merged['norm_HeartRate'], color='darkorange')\n",
    "        plt.ylabel('HeartRate', rotation=0)\n",
    "        ax3.yaxis.set_label_coords(-0.05,0.5)\n",
    "\n",
    "        ax4 = fig.add_subplot(gs[6:10, 0], sharex=ax1)\n",
    "        plt.plot(df_merged['norm_Motion'])\n",
    "        plt.ylabel('Motion', rotation=0)\n",
    "        ax4.yaxis.set_label_coords(-0.05,0.5)\n",
    "\n",
    "        ax5 = fig.add_subplot(gs[10:14, 0])\n",
    "        plt.plot(df_merged['norm_Temperature'], color='k')\n",
    "        plt.ylabel('Temperature', rotation=0)\n",
    "        plt.xlabel('time')\n",
    "        plt.xlim(0,last_bin)\n",
    "        ax5.yaxis.set_label_coords(-0.05,0.5)\n",
    "\n",
    "\n",
    "        for ax in [ax1, ax2]:\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['left'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['bottom'].set_visible(False)\n",
    "            ax.get_xaxis().set_ticks([])\n",
    "            ax.get_yaxis().set_ticks([])\n",
    "\n",
    "        for ax in [ax3, ax4]:\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "        for ax in [ax5]:\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)   \n",
    "\n",
    "        plt.tight_layout()    \n",
    "        \n",
    "        if SAVE:\n",
    "            plt.savefig('/home/ds/DCL/Defensive_states/{}/{}_{}.png'.format(folder, mouse, session), dpi=300)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-quantum",
   "metadata": {},
   "source": [
    "## Plot all cluster from 1 run - not checking for stable clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_mice = df['Animal_ID'].unique()\n",
    "#l_mice = [df['Animal_ID'].unique()[1]]\n",
    "#l_mice = ['175_F4-19']\n",
    "\n",
    "folder = 'Plots_states_over_raw_data_all_mice_ceiling_reduced3_all_run1_cluster'\n",
    "\n",
    "\n",
    "SAVE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_matching_triplets_r1 = [elem[0][1] for elem in l_matching_triplets_abc]\n",
    "\n",
    "\n",
    "for mouse in l_mice:\n",
    "    l_sessions = df.loc[df['Animal_ID'] == mouse, 'Session'].unique()\n",
    "    #l_sessions = ['CD2']\n",
    "    \n",
    "    for session in l_sessions:\n",
    "        # Select the corresponding session data\n",
    "        df_session = df.loc[(df['Animal_ID'] == mouse) & (df['Session'] == session)].copy()\n",
    "        \n",
    "        # Since df was entirely cleaned from NaNs already, we have to recreate the gaps so that they appear in the plotted traces and are not connected\n",
    "        last_bin = df_session['Bin'].max()\n",
    "        data = {'Bin': [int(elem) for elem in np.linspace(1, last_bin, last_bin)]}\n",
    "        df_bins = pd.DataFrame (data=data)\n",
    "        df_merged = pd.merge(df_session, df_bins, on='Bin', how='right')\n",
    "        \n",
    "        # To show only relevant behaviors and relevant states, we will create corresponding columns that can be used to specify the hue\n",
    "        df_merged['matching_cluster_ids'] = np.NaN\n",
    "        df_merged.loc[df_merged['Cluster_cont_r1'].isin(l_matching_triplets_r1), 'matching_cluster_ids'] = df_merged['Cluster_cont_r1']\n",
    "\n",
    "        l_selected_behaviors = ['Immobility', 'Grooming', 'Rearing', 'Flight', 'StretchAttend', 'TailRattling', 'HeadDips']\n",
    "        df_merged['selected_behaviors'] = np.NaN\n",
    "        df_merged.loc[df_merged['behaviors'].isin(l_selected_behaviors), 'selected_behaviors'] = df_merged['behaviors']\n",
    "        \n",
    "        # Also, we need some fixed y-value for the scatterplots of behaviors and states:\n",
    "        df_merged['plain_one'] = 1\n",
    "        \n",
    "        # Now we are ready to create the figure: \n",
    "        fig = plt.figure(figsize=(20, 8), facecolor='white')\n",
    "        gs = fig.add_gridspec(14,1)\n",
    "\n",
    "        ax1 = fig.add_subplot(gs[0,0])\n",
    "        sns.scatterplot(data=df_merged, x='Bin', y='plain_one', hue='selected_behaviors', marker=\"|\", palette='colorblind', legend=False, s=100, ax=ax1)\n",
    "        plt.title('{} during {}'.format(mouse, session))\n",
    "        plt.xlim(0,last_bin)\n",
    "        plt.ylabel('behavior', rotation=0)\n",
    "        plt.xlabel('')\n",
    "        ax1.yaxis.set_label_coords(-0.05,0.3)\n",
    "\n",
    "        ax2 = fig.add_subplot(gs[1, 0], sharex=ax1)\n",
    "        sns.scatterplot(data=df_merged.loc[df_merged['Cluster_cont_r1'] != -1], x='Bin', y='plain_one', hue='Cluster_cont_r1', marker=\"|\", palette='Spectral', legend=False, s=100, ax=ax2)\n",
    "        plt.ylabel('state', rotation=0)\n",
    "        plt.xlabel('')\n",
    "        ax2.yaxis.set_label_coords(-0.05,0.3)\n",
    "\n",
    "        ax3 = fig.add_subplot(gs[2:6, 0], sharex=ax1)\n",
    "        plt.plot(df_merged['norm_HeartRate'], color='darkorange')\n",
    "        plt.ylabel('HeartRate', rotation=0)\n",
    "        ax3.yaxis.set_label_coords(-0.05,0.5)\n",
    "\n",
    "        ax4 = fig.add_subplot(gs[6:10, 0], sharex=ax1)\n",
    "        plt.plot(df_merged['norm_Motion'])\n",
    "        plt.ylabel('Motion', rotation=0)\n",
    "        ax4.yaxis.set_label_coords(-0.05,0.5)\n",
    "\n",
    "        ax5 = fig.add_subplot(gs[10:14, 0])\n",
    "        plt.plot(df_merged['norm_Temperature'], color='k')\n",
    "        plt.ylabel('Temperature', rotation=0)\n",
    "        plt.xlabel('time')\n",
    "        plt.xlim(0,last_bin)\n",
    "        ax5.yaxis.set_label_coords(-0.05,0.5)\n",
    "\n",
    "\n",
    "        for ax in [ax1, ax2]:\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['left'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['bottom'].set_visible(False)\n",
    "            ax.get_xaxis().set_ticks([])\n",
    "            ax.get_yaxis().set_ticks([])\n",
    "\n",
    "        for ax in [ax3, ax4]:\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "        for ax in [ax5]:\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)   \n",
    "\n",
    "        plt.tight_layout()    \n",
    "        \n",
    "        if SAVE:\n",
    "            plt.savefig('/home/ds/DCL/Defensive_states/{}/{}_{}.png'.format(folder, mouse, session), dpi=300)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-arbor",
   "metadata": {},
   "source": [
    "## Plot all bins that belong to a stable cluster (in any of the three runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_matching_triplets_r1 = [elem[0][1] for elem in l_matching_triplets_abc]\n",
    "l_unique_matching_triples_r1 = list(set(l_matching_triplets_r1))\n",
    "\n",
    "l_matching_triplets_r2 = [elem[1][1] for elem in l_matching_triplets_abc]\n",
    "l_unique_matching_triples_r2 = list(set(l_matching_triplets_r2))\n",
    "\n",
    "l_matching_triplets_r3 = [elem[2][1] for elem in l_matching_triplets_abc]\n",
    "l_unique_matching_triples_r3 = list(set(l_matching_triplets_r3))\n",
    "\n",
    "\n",
    "\n",
    "ID = '0'\n",
    "l_matching_triplets_abc_fused = []\n",
    "\n",
    "for unique_id_r1 in l_unique_matching_triples_r1:\n",
    "    for triplet_combo in l_matching_triplets_abc:\n",
    "        if triplet_combo[0][1] == unique_id_r1:\n",
    "            l_matching_triplets_abc_fused.append(triplet_combo + [['fused', ID]])\n",
    "    ID = str(int(ID) + 1)\n",
    "\n",
    "for unique_id_r2 in l_unique_matching_triples_r2:\n",
    "    l_matching_fused_ids = []\n",
    "    for combo in l_matching_triplets_abc_fused:\n",
    "        if combo[1][1] == unique_id_r2:\n",
    "            l_matching_fused_ids.append(combo[3][1])\n",
    "    if len(l_matching_fused_ids) > 1:\n",
    "            for combi in l_matching_triplets_abc_fused:\n",
    "                if combi[1][1] == unique_id_r2:\n",
    "                    combi[3][1] = l_matching_fused_ids[0]\n",
    "\n",
    "                    \n",
    "for unique_id_r3 in l_unique_matching_triples_r3:\n",
    "    l_matching_fused_ids = []\n",
    "    for combo in l_matching_triplets_abc_fused:\n",
    "        if combo[2][1] == unique_id_r3:\n",
    "            l_matching_fused_ids.append(combo[3][1])\n",
    "    if len(l_matching_fused_ids) > 1:\n",
    "            for combi in l_matching_triplets_abc_fused:\n",
    "                if combi[2][1] == unique_id_r3:\n",
    "                    combi[3][1] = l_matching_fused_ids[0]\n",
    "\n",
    "                    \n",
    "for elem in l_matching_triplets_abc_fused:\n",
    "    elem[3] = tuple(elem[3])\n",
    "\n",
    "    \n",
    "    \n",
    "df['fused_stable_cluster_ids'] = np.NaN\n",
    "\n",
    "for unique_id_r1 in l_unique_matching_triples_r1:\n",
    "    for elem in l_matching_triplets_abc_fused:\n",
    "        if elem[0][1] == unique_id_r1:\n",
    "            fused_id = elem[3][1]\n",
    "    df.loc[df['Cluster_cont_r1'] == unique_id_r1, 'fused_stable_cluster_ids'] = fused_id\n",
    "    \n",
    "for unique_id_r2 in l_unique_matching_triples_r2:\n",
    "    for elem in l_matching_triplets_abc_fused:\n",
    "        if elem[1][1] == unique_id_r2:\n",
    "            fused_id = elem[3][1]\n",
    "    df.loc[df['Cluster_cont_r2'] == unique_id_r2, 'fused_stable_cluster_ids'] = fused_id\n",
    "    \n",
    "for unique_id_r3 in l_unique_matching_triples_r3:\n",
    "    for elem in l_matching_triplets_abc_fused:\n",
    "        if elem[2][1] == unique_id_r3:\n",
    "            fused_id = elem[3][1]\n",
    "    df.loc[df['Cluster_cont_r3'] == unique_id_r3, 'fused_stable_cluster_ids'] = fused_id\n",
    "\n",
    "    \n",
    "df['fused_stable_cluster_ids'].unique()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_mice = df['Animal_ID'].unique()\n",
    "#_mice = [df['Animal_ID'].unique()[3]]\n",
    "#l_mice = ['175_F4-11']\n",
    "\n",
    "folder = 'Plots_states_over_raw_data_all_mice_ceiling_reduced3_fused_runs'\n",
    "\n",
    "\n",
    "SAVE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mouse in l_mice:\n",
    "    l_sessions = df.loc[df['Animal_ID'] == mouse, 'Session'].unique()\n",
    "    #l_sessions = ['CD1']\n",
    "    \n",
    "    for session in l_sessions:\n",
    "        # Select the corresponding session data\n",
    "        df_session = df.loc[(df['Animal_ID'] == mouse) & (df['Session'] == session)].copy()\n",
    "        \n",
    "        # Since df was entirely cleaned from NaNs already, we have to recreate the gaps so that they appear in the plotted traces and are not connected\n",
    "        last_bin = df_session['Bin'].max()\n",
    "        data = {'Bin': [int(elem) for elem in np.linspace(1, last_bin, last_bin)]}\n",
    "        df_bins = pd.DataFrame (data=data)\n",
    "        df_merged = pd.merge(df_session, df_bins, on='Bin', how='right')\n",
    "        \n",
    "        # To show only relevant behaviors and relevant states, we will create corresponding columns that can be used to specify the hue\n",
    "        #df_merged['matching_cluster_ids'] = np.NaN\n",
    "        #df_merged.loc[df_merged['Cluster_cont_r1'].isin(l_matching_triplets_r1), 'matching_cluster_ids'] = df_merged['Cluster_cont_r1']\n",
    "\n",
    "        l_selected_behaviors = ['Immobility', 'Grooming', 'Rearing', 'Flight', 'StretchAttend', 'TailRattling', 'HeadDips']\n",
    "        df_merged['selected_behaviors'] = np.NaN\n",
    "        df_merged.loc[df_merged['behaviors'].isin(l_selected_behaviors), 'selected_behaviors'] = df_merged['behaviors']\n",
    "        \n",
    "        # Also, we need some fixed y-value for the scatterplots of behaviors and states:\n",
    "        df_merged['plain_one'] = 1\n",
    "        \n",
    "        # Now we are ready to create the figure: \n",
    "        fig = plt.figure(figsize=(20, 8), facecolor='white')\n",
    "        gs = fig.add_gridspec(14,1)\n",
    "\n",
    "        ax1 = fig.add_subplot(gs[0,0])\n",
    "        sns.scatterplot(data=df_merged, x='Bin', y='plain_one', hue='selected_behaviors', marker=\"|\", palette='colorblind', legend=False, s=100, ax=ax1)\n",
    "        plt.title('{} during {}'.format(mouse, session))\n",
    "        plt.xlim(0,last_bin)\n",
    "        plt.ylabel('behavior', rotation=0)\n",
    "        plt.xlabel('')\n",
    "        ax1.yaxis.set_label_coords(-0.05,0.3)\n",
    "\n",
    "        ax2 = fig.add_subplot(gs[1, 0], sharex=ax1)\n",
    "        sns.scatterplot(data=df_merged, x='Bin', y='plain_one', hue='fused_stable_cluster_ids', marker=\"|\", palette='Spectral', legend=False, s=100, ax=ax2)\n",
    "        plt.ylabel('state', rotation=0)\n",
    "        plt.xlabel('')\n",
    "        ax2.yaxis.set_label_coords(-0.05,0.3)\n",
    "\n",
    "        ax3 = fig.add_subplot(gs[2:6, 0], sharex=ax1)\n",
    "        plt.plot(df_merged['norm_HeartRate'], color='darkorange')\n",
    "        plt.ylabel('HeartRate', rotation=0)\n",
    "        ax3.yaxis.set_label_coords(-0.05,0.5)\n",
    "\n",
    "        ax4 = fig.add_subplot(gs[6:10, 0], sharex=ax1)\n",
    "        plt.plot(df_merged['norm_Motion'])\n",
    "        plt.ylabel('Motion', rotation=0)\n",
    "        ax4.yaxis.set_label_coords(-0.05,0.5)\n",
    "\n",
    "        ax5 = fig.add_subplot(gs[10:14, 0])\n",
    "        plt.plot(df_merged['norm_Temperature'], color='k')\n",
    "        plt.ylabel('Temperature', rotation=0)\n",
    "        plt.xlabel('time')\n",
    "        plt.xlim(0,last_bin)\n",
    "        ax5.yaxis.set_label_coords(-0.05,0.5)\n",
    "\n",
    "\n",
    "        for ax in [ax1, ax2]:\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['left'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['bottom'].set_visible(False)\n",
    "            ax.get_xaxis().set_ticks([])\n",
    "            ax.get_yaxis().set_ticks([])\n",
    "\n",
    "        for ax in [ax3, ax4]:\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "        for ax in [ax5]:\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)   \n",
    "\n",
    "        plt.tight_layout()    \n",
    "        \n",
    "        if SAVE:\n",
    "            plt.savefig('/home/ds/DCL/Defensive_states/{}/{}_{}.png'.format(folder, mouse, session), dpi=300)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-tanzania",
   "metadata": {},
   "source": [
    "## Check the distribution of the detected stable clusters\n",
    "\n",
    "Do the mice display a larger variety of states in OF and EPM, compared to CD1 and CD2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_clusters_per_mouse = {'Cluster_count': [],\n",
    "                        'Session': [],\n",
    "                        'Animal_ID': []}\n",
    "\n",
    "for mouse in df['Animal_ID'].unique():\n",
    "    for session in ['OF', 'EPM', 'CD1', 'CD2']:\n",
    "        d_clusters_per_mouse['Cluster_count'].append(df.loc[(df['Animal_ID'] == mouse) & (df['Session'] == session), 'fused_stable_cluster_ids'].unique().shape[0] - 1) # quick and dirty to \"remove\" nan\n",
    "        d_clusters_per_mouse['Session'].append(session)\n",
    "        d_clusters_per_mouse['Animal_ID'].append(mouse)\n",
    "        \n",
    "    d_clusters_per_mouse['Cluster_count'].append(df.loc[(df['Animal_ID'] == mouse) & (df['Session'].isin(['OF', 'EPM'])), 'fused_stable_cluster_ids'].unique().shape[0] - 1)\n",
    "    d_clusters_per_mouse['Session'].append('Explorative')\n",
    "    d_clusters_per_mouse['Animal_ID'].append(mouse)\n",
    "    \n",
    "    d_clusters_per_mouse['Cluster_count'].append(df.loc[(df['Animal_ID'] == mouse) & (df['Session'].isin(['CD1', 'CD2'])), 'fused_stable_cluster_ids'].unique().shape[0] - 1)\n",
    "    d_clusters_per_mouse['Session'].append('CondFlight')\n",
    "    d_clusters_per_mouse['Animal_ID'].append(mouse)\n",
    "    \n",
    "df_clusters_per_mouse = pd.DataFrame(data=d_clusters_per_mouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6), facecolor='white')\n",
    "\n",
    "sns.boxplot(data=df_clusters_per_mouse, x='Session', y='Cluster_count')\n",
    "sns.stripplot(data=df_clusters_per_mouse, x='Session', y='Cluster_count', color='k')\n",
    "plt.title('Different states found in each session [per mouse]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-bahamas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-saver",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-possession",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-gregory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "strategic-triple",
   "metadata": {},
   "source": [
    "## Special stuff added for Ninas Institute PR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_column = 'Cluster_cont_r2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentage_of_bins_per_behavior(df_temp):\n",
    "    perc_Immobility = df_temp.loc[df_temp['behaviors'] == 'Immobility'].shape[0] / df_temp.shape[0] * 100\n",
    "    perc_StretchAttend = df_temp.loc[df_temp['behaviors'] == 'StretchAttend'].shape[0] / df_temp.shape[0] * 100\n",
    "    perc_Grooming = df_temp.loc[df_temp['behaviors'] == 'Grooming'].shape[0] / df_temp.shape[0] * 100\n",
    "    perc_TailRattling = df_temp.loc[df_temp['behaviors'] == 'TailRattling'].shape[0] / df_temp.shape[0] * 100\n",
    "    perc_Flight = df_temp.loc[df_temp['behaviors'] == 'Flight'].shape[0] / df_temp.shape[0] * 100\n",
    "    perc_Rearing = df_temp.loc[df_temp['behaviors'] == 'Rearing'].shape[0] / df_temp.shape[0] * 100\n",
    "    \n",
    "    #perc_Struggle = df_temp.loc[df_temp['behaviors'] == 'Struggle'].shape[0] / df_temp.shape[0] * 100\n",
    "    perc_HeadDips = df_temp.loc[df_temp['behaviors'] == 'HeadDips'].shape[0] / df_temp.shape[0] * 100\n",
    "    #perc_OpenRearing = df_temp.loc[df_temp['behaviors'] == 'OpenRearing'].shape[0] / df_temp.shape[0] * 100\n",
    "    \n",
    "    perc_NoScore = df_temp.loc[df_temp['behaviors'] == 'No score'].shape[0] / df_temp.shape[0] * 100\n",
    "    perc_Remaining = df_temp.loc[df_temp['behaviors'] == 'Remaining'].shape[0] / df_temp.shape[0] * 100\n",
    "    perc_multiple = df_temp.loc[df_temp['behaviors'] == 'multiple'].shape[0] / df_temp.shape[0] * 100\n",
    "    dict_perc_behaviors = {'Immobility': perc_Immobility,\n",
    "                          'StretchAttend': perc_StretchAttend,\n",
    "                          'Grooming': perc_Grooming, \n",
    "                          'TailRattling': perc_TailRattling, \n",
    "                          'Flight': perc_Flight,\n",
    "                          'Rearing': perc_Rearing,\n",
    "                          #'OpenRearing': perc_OpenRearing,\n",
    "                          'HeadDips': perc_HeadDips,\n",
    "                          #'Struggle': perc_Struggle,\n",
    "                          'No Score': perc_NoScore, \n",
    "                          'Remaining': perc_Remaining,\n",
    "                          'multiple': perc_multiple}\n",
    "    return dict_perc_behaviors\n",
    "\n",
    "\n",
    "l_clusters = list(df.loc[df['Exclude'] == False, cluster_column].unique())\n",
    "l_clusters.remove(-1)\n",
    "\n",
    "l_enriched_dfs = []\n",
    "\n",
    "for cluster_id in l_clusters:\n",
    "    dict_cluster = get_percentage_of_bins_per_behavior(df.loc[(df['Exclude'] == False) & (df[cluster_column] == cluster_id)])\n",
    "    dict_all_data = get_percentage_of_bins_per_behavior(df.loc[df['Exclude'] == False])\n",
    "\n",
    "    zip_object = zip(dict_cluster.values(), dict_all_data.values())\n",
    "\n",
    "    l_enriched = []\n",
    "    for perc_cluster, perc_all_data in zip_object:\n",
    "        l_enriched.append(perc_cluster/perc_all_data)\n",
    "\n",
    "    dict_enriched = {'Cluster_ID': [cluster_id]}\n",
    "    for i in range(len(l_enriched)):\n",
    "        dict_enriched[list(dict_all_data.keys())[i]] = [l_enriched[i]]\n",
    "\n",
    "    df_enriched = pd.DataFrame(data=dict_enriched)\n",
    "    df_enriched = df_enriched.set_index('Cluster_ID', drop=True)\n",
    "\n",
    "    l_enriched_dfs.append(df_enriched)\n",
    "    \n",
    "df_all_clusters = pd.concat(l_enriched_dfs)\n",
    "\n",
    "l_cluster_sizes = []\n",
    "for idx in df_all_clusters.index:\n",
    "    l_cluster_sizes.append(df.loc[(df['Exclude'] == False) & (df[cluster_column] == idx)].shape[0])\n",
    "\n",
    "df_all_clusters['Cluster_size'] = l_cluster_sizes\n",
    "df_all_clusters.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which behavior should be used to sort the heatmaps?\n",
    "\n",
    "sort_behavior = 'Immobility'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heatmap = df_all_clusters.copy()\n",
    "\n",
    "for behavior in ['StretchAttend', 'Grooming', 'TailRattling', 'Flight', 'Rearing', 'HeadDips']:\n",
    "    df_heatmap[behavior] = np.sqrt(df_heatmap[behavior])\n",
    "\n",
    "fig = plt.figure(figsize=(20,15), facecolor='w')\n",
    "gs = fig.add_gridspec(15,7)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.05) \n",
    "\n",
    "cbar_1 = fig.add_subplot(gs[14,0])\n",
    "cbar_2 = fig.add_subplot(gs[14,1])\n",
    "cbar_3 = fig.add_subplot(gs[14,2])\n",
    "cbar_4 = fig.add_subplot(gs[14,3])\n",
    "cbar_5 = fig.add_subplot(gs[14,4])\n",
    "cbar_6 = fig.add_subplot(gs[14,5])\n",
    "cbar_7 = fig.add_subplot(gs[14,6])\n",
    "#cbar_8 = fig.add_subplot(gs[14,7])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0:13,0])\n",
    "sns.heatmap(df_heatmap.sort_values(by = sort_behavior, ascending=False).iloc[:, :1], center=1, cbar_kws={\"orientation\": \"horizontal\"}, cmap='vlag', vmin=0, vmax=df_heatmap['Immobility'].max(), cbar_ax=cbar_1)\n",
    "plt.tick_params(bottom=False, labelbottom=False, labeltop=True, left='Full')\n",
    "\n",
    "fig.add_subplot(gs[0:13,1])\n",
    "sns.heatmap(df_heatmap.sort_values(by = sort_behavior, ascending=False).iloc[:, 1:2], center=1, cbar_kws={\"orientation\": \"horizontal\"}, cmap='vlag', vmin=0, vmax=df_heatmap['StretchAttend'].max(), cbar_ax=cbar_2)\n",
    "plt.ylabel('')\n",
    "plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False, labeltop=True)\n",
    "\n",
    "fig.add_subplot(gs[0:13,2])\n",
    "sns.heatmap(df_heatmap.sort_values(by = sort_behavior, ascending=False).iloc[:, 2:3], center=1, cbar_kws={\"orientation\": \"horizontal\"}, cmap='vlag', vmin=0, vmax=df_heatmap['Grooming'].max(), cbar_ax=cbar_3)\n",
    "plt.ylabel('')\n",
    "plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False, labeltop=True)\n",
    "\n",
    "fig.add_subplot(gs[0:13,3])\n",
    "sns.heatmap(df_heatmap.sort_values(by = sort_behavior, ascending=False).iloc[:, 3:4], center=1, cbar_kws={\"orientation\": \"horizontal\"}, cmap='vlag', vmin=0, vmax=df_heatmap['TailRattling'].max(), cbar_ax=cbar_4)\n",
    "plt.ylabel('')\n",
    "plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False, labeltop=True)\n",
    "\n",
    "fig.add_subplot(gs[0:13,4])\n",
    "sns.heatmap(df_heatmap.sort_values(by = sort_behavior, ascending=False).iloc[:, 4:5], center=1, cbar_kws={\"orientation\": \"horizontal\"}, cmap='vlag', vmin=0, vmax=df_heatmap['Flight'].max(), cbar_ax=cbar_5)\n",
    "plt.ylabel('')\n",
    "plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False, labeltop=True)\n",
    "\n",
    "fig.add_subplot(gs[0:13,5])\n",
    "sns.heatmap(df_heatmap.sort_values(by = sort_behavior, ascending=False).iloc[:, 5:6], center=1, cbar_kws={\"orientation\": \"horizontal\"}, cmap='vlag', vmin=0, vmax=df_heatmap['Rearing'].max(), cbar_ax=cbar_6)\n",
    "plt.ylabel('')\n",
    "plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False, labeltop=True)\n",
    "\n",
    "fig.add_subplot(gs[0:13,6])\n",
    "sns.heatmap(df_heatmap.sort_values(by = sort_behavior, ascending=False).iloc[:, 6:7], center=1, cbar_kws={\"orientation\": \"horizontal\"}, cmap='vlag', vmin=0, vmax=df_heatmap['HeadDips'].max(), cbar_ax=cbar_7)\n",
    "plt.ylabel('')\n",
    "plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False, labeltop=True)\n",
    "\n",
    "#fig.add_subplot(gs[0:13,7])\n",
    "#sns.heatmap(df_heatmap.sort_values(by = sort_behavior, ascending=False).iloc[:, 5:6], center=1, cbar_kws={\"orientation\": \"horizontal\"}, cmap='vlag', vmin=0, vmax=df_heatmap['HeadDips'].max(), cbar_ax=cbar_6)\n",
    "#plt.ylabel('')\n",
    "#plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False, labeltop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.savefig('Heatmap_clusters_sorted_by_immobility.png', dpi=300)\n",
    "plt.suptitle('Square-root transformed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-accreditation",
   "metadata": {},
   "source": [
    "### Use all clusters of that run and sort them (don´t check for matching triplets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_sorted = list(df_all_clusters.sort_values(by=[sort_behavior], ascending=False).index)\n",
    "\n",
    "if len(l_sorted) > 24:\n",
    "    l_clusters = l_sorted[:25]\n",
    "else:\n",
    "    l_clusters = l_sorted\n",
    "    \n",
    "df_inspect = df.loc[df[cluster_column].isin(l_clusters)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-warren",
   "metadata": {},
   "source": [
    "### Use all clusters of that run and sort them AND check for matching triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_sorted = list(df_all_clusters.sort_values(by=[sort_behavior], ascending=False).index)\n",
    "\n",
    "l_matching_clusters_run_2 = [elem[1][1] for elem in l_matching_triplets_abc]\n",
    "\n",
    "l_matchings_sorted = [elem for elem in l_sorted if elem in l_matching_clusters_run_2]\n",
    "\n",
    "if len(l_matchings_sorted) > 24:\n",
    "    l_clusters = l_matchings_sorted[:25]\n",
    "else:\n",
    "    l_clusters = l_matchings_sorted\n",
    "    \n",
    "df_inspect = df.loc[df[cluster_column].isin(l_clusters)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-parent",
   "metadata": {},
   "source": [
    "### Plot only selected clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 1:\n",
    "l_clusters = [44, 63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 2:\n",
    "l_clusters = [120, 43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inspect = df.loc[df[cluster_column].isin(l_clusters)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_n_mice = []\n",
    "\n",
    "for cluster_id in l_clusters:\n",
    "    l_n_mice.append(df.loc[df[cluster_column] == cluster_id, 'Animal_ID'].unique().shape[0])\n",
    "    \n",
    "fig = plt.figure(figsize=(15, 8), facecolor='w')\n",
    "\n",
    "sns.barplot(x=l_clusters, y=l_n_mice, order=l_clusters)\n",
    "plt.ylabel('Number of mice')\n",
    "plt.xlabel('Cluster_ID')\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cluster_column\n",
    "norm = 'norm_'\n",
    "\n",
    "fig = plt.figure(figsize=(28, 30), facecolor='w')\n",
    "gs = fig.add_gridspec(3,3)\n",
    "plt.subplots_adjust(hspace=0.4) \n",
    "\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "sns.boxplot(data=df_inspect, x=X, y=norm + 'HeartRate', ax=ax1, order=l_clusters)\n",
    "plt.title('Heart Rate', fontsize=20)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('normalized measure')\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0,1])\n",
    "sns.boxplot(data=df_inspect, x=X, y=norm + 'HR_High_Amp', ax=ax2, order=l_clusters)\n",
    "plt.title('Amplitude of high HR frequency band', fontsize=20)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('normalized measure')\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0,2])\n",
    "sns.boxplot(data=df_inspect, x=X, y=norm + 'HR_CoV_10s', ax=ax3, order=l_clusters)\n",
    "plt.title('HR coefficient of variation [10s window]', fontsize=20)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('normalized measure')\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1,0])\n",
    "sns.boxplot(data=df_inspect, x=X, y='norm_Motion', ax=ax4, order=l_clusters)\n",
    "plt.title('Motion', fontsize=20)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('normalized measure')\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "ax5 = fig.add_subplot(gs[1,1])\n",
    "sns.boxplot(data=df_inspect, x=X, y='norm_Speed', ax=ax5, order=l_clusters)\n",
    "plt.title('Speed', fontsize=20)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('normalized measure')\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "ax6 = fig.add_subplot(gs[1,2])\n",
    "sns.boxplot(data=df_inspect, x=X, y='norm_AreaExplored_sqrt', ax=ax6, order=l_clusters)\n",
    "plt.title('AreaExplored - sqrt. transformed', fontsize=20)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('normalized measure')\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "\n",
    "ax7 = fig.add_subplot(gs[2,0])\n",
    "sns.boxplot(data=df_inspect, x=X, y=norm + 'Temperature_s1', ax=ax7, order=l_clusters)\n",
    "plt.title('Temperature tail segment 1', fontsize=20)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('normalized measure')\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "ax8 = fig.add_subplot(gs[2,1])\n",
    "sns.boxplot(data=df_inspect, x=X, y=norm + 'Temperature_s3', ax=ax8, order=l_clusters)\n",
    "plt.title('Temperature tail segment 3', fontsize=20)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('normalized measure')\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "\n",
    "\n",
    "#plt.tight_layout()\n",
    "\n",
    "#plt.savefig('5_Immobility_cluster_boxplots.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pie_chart_infos(cluster_id):\n",
    "    df_temp = df_inspect.loc[df_inspect[X] == cluster_id, ['behaviors', 'Bin']].copy()\n",
    "    total_bins = df_temp.shape[0]\n",
    "    behaviors_perc = {'Immobility': round(df_temp.loc[df_temp['behaviors'] == 'Immobility'].shape[0] / total_bins * 100, 2),\n",
    "                      'Stretch Attend': round(df_temp.loc[df_temp['behaviors'] == 'StretchAttend'].shape[0] / total_bins * 100, 2),\n",
    "                      'No score': round(df_temp.loc[df_temp['behaviors'] == 'No score'].shape[0] / total_bins * 100, 2),\n",
    "                      'Grooming': round(df_temp.loc[df_temp['behaviors'] == 'Grooming'].shape[0] / total_bins * 100, 2),\n",
    "                      'Remaining': round(df_temp.loc[df_temp['behaviors'] == 'Remaining'].shape[0] / total_bins * 100, 2),\n",
    "                      'Tail Rattling': round(df_temp.loc[df_temp['behaviors'] == 'TailRattling'].shape[0] / total_bins * 100, 2),\n",
    "                      'Flight': round(df_temp.loc[df_temp['behaviors'] == 'Flight'].shape[0] / total_bins * 100, 2),\n",
    "                      'Rearing': round(df_temp.loc[df_temp['behaviors'] == 'Rearing'].shape[0] / total_bins * 100, 2),\n",
    "                      \n",
    "                      #'Struggle': round(df_temp.loc[df_temp['behaviors'] == 'Struggle'].shape[0] / total_bins * 100, 2),\n",
    "                      #'OpenRearing': round(df_temp.loc[df_temp['behaviors'] == 'OpenRearing'].shape[0] / total_bins * 100, 2),\n",
    "                      'HeadDips': round(df_temp.loc[df_temp['behaviors'] == 'HeadDips'].shape[0] / total_bins * 100, 2),\n",
    "                      \n",
    "                      'Multiple': round(df_temp.loc[df_temp['behaviors'] == 'multiple'].shape[0] / total_bins * 100, 2)}\n",
    "    labels = []\n",
    "    sizes = []\n",
    "    for key in behaviors_perc.keys():\n",
    "        if behaviors_perc[key] > 0.5:\n",
    "            labels.append(key)\n",
    "            sizes.append(behaviors_perc[key])\n",
    "    return sizes, labels\n",
    "\n",
    "def calculate_pie_chart_infos_average():\n",
    "    df_temp = df.copy()\n",
    "    total_bins = df_temp.shape[0]\n",
    "    behaviors_perc = {'Immobility': round(df_temp.loc[df_temp['behaviors'] == 'Immobility'].shape[0] / total_bins * 100, 2),\n",
    "                      'Stretch Attend': round(df_temp.loc[df_temp['behaviors'] == 'StretchAttend'].shape[0] / total_bins * 100, 2),\n",
    "                      'No score': round(df_temp.loc[df_temp['behaviors'] == 'No score'].shape[0] / total_bins * 100, 2),\n",
    "                      'Grooming': round(df_temp.loc[df_temp['behaviors'] == 'Grooming'].shape[0] / total_bins * 100, 2),\n",
    "                      'Remaining': round(df_temp.loc[df_temp['behaviors'] == 'Remaining'].shape[0] / total_bins * 100, 2),\n",
    "                      'Tail Rattling': round(df_temp.loc[df_temp['behaviors'] == 'TailRattling'].shape[0] / total_bins * 100, 2),\n",
    "                      'Flight': round(df_temp.loc[df_temp['behaviors'] == 'Flight'].shape[0] / total_bins * 100, 2),\n",
    "                      'Rearing': round(df_temp.loc[df_temp['behaviors'] == 'Rearing'].shape[0] / total_bins * 100, 2),\n",
    "                      \n",
    "                      #'OpenRearing': round(df_temp.loc[df_temp['behaviors'] == 'OpenRearing'].shape[0] / total_bins * 100, 2),\n",
    "                      #'Struggle': round(df_temp.loc[df_temp['behaviors'] == 'Struggle'].shape[0] / total_bins * 100, 2),\n",
    "                      'HeadDips': round(df_temp.loc[df_temp['behaviors'] == 'HeadDips'].shape[0] / total_bins * 100, 2),\n",
    "                      \n",
    "                      'Multiple': round(df_temp.loc[df_temp['behaviors'] == 'multiple'].shape[0] / total_bins * 100, 2)}\n",
    "    labels = []\n",
    "    sizes = []\n",
    "    for key in behaviors_perc.keys():\n",
    "        if behaviors_perc[key] > 0.5:\n",
    "            labels.append(key)\n",
    "            sizes.append(behaviors_perc[key])\n",
    "    return sizes, labels\n",
    "        \n",
    "fig = plt.figure(figsize=(22, 55), facecolor='w')\n",
    "gs = fig.add_gridspec(6,4)\n",
    "plt.subplots_adjust(hspace=1) \n",
    "\n",
    "\n",
    "cluster_id = 0\n",
    "for row in [0, 1, 2, 3, 4, 5]:\n",
    "    if row < 5:\n",
    "        for column in [0, 1, 2, 3]:\n",
    "            fig.add_subplot(gs[row,column])\n",
    "            sizes, labels = calculate_pie_chart_infos(l_clusters[cluster_id])\n",
    "            plt.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "                    shadow=False, startangle=90, rotatelabels=True)\n",
    "            plt.axis('equal')\n",
    "            plt.title('Cluster ' + str(l_clusters[cluster_id]) + '\\n' + str(df_all_clusters.loc[l_clusters[cluster_id],'Cluster_size']) + ' bins', pad=80, fontsize=24)\n",
    "            cluster_id = cluster_id + 1\n",
    "    if row == 5:     \n",
    "        for column in [0, 1, 2]:\n",
    "            fig.add_subplot(gs[row,column])\n",
    "            sizes, labels = calculate_pie_chart_infos(l_clusters[cluster_id])\n",
    "            plt.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "                    shadow=False, startangle=90, rotatelabels=True)\n",
    "            plt.axis('equal')\n",
    "            plt.title('Cluster ' + str(l_clusters[cluster_id]) + '\\n' + str(df_all_clusters.loc[l_clusters[cluster_id],'Cluster_size']) + ' bins', pad=80, fontsize=24)\n",
    "            cluster_id = cluster_id + 1\n",
    "        \n",
    "        fig.add_subplot(gs[row,3])\n",
    "        sizes, labels = calculate_pie_chart_infos_average()\n",
    "        plt.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "                shadow=False, startangle=90, rotatelabels=True)\n",
    "        plt.axis('equal')\n",
    "        plt.title('Entire DataFrame \\n' + str(df.shape[0]) + ' bins', pad=90, fontsize=24)\n",
    "        cluster_id = cluster_id + 1\n",
    "\n",
    "#plt.savefig('10_Immobility_cluster_piecharts_with_bins.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-island",
   "metadata": {},
   "source": [
    "## Normalization of time across sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Session'] == 'OF', 'Times'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_session_times = {}\n",
    "\n",
    "for session in df['Session'].unique():\n",
    "    max_session_times[session] = df.loc[df['Session'] == session, 'Times'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_session_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_mice = df['Animal_ID'].unique()\n",
    "for mouse in l_mice:\n",
    "    l_sessions = df.loc[df['Animal_ID'] == mouse, 'Session'].unique()\n",
    "    for session in l_sessions:\n",
    "        df.loc[(df['Animal_ID'] == mouse) & (df['Session'] == session), 'norm_Times'] = df['Times'] / max_session_times[session]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Times'] == 5.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3), facecolor='w')\n",
    "gs = fig.add_gridspec(1,1)\n",
    "\n",
    "stripplot = True\n",
    "\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "sns.boxplot(data=df.loc[df[cluster_column].isin(l_clusters)], x='norm_Times', y=cluster_column, orient='h', order=l_clusters, ax=ax1, palette=['magenta', 'darkgreen'], fliersize=0)\n",
    "\n",
    "sns.stripplot(data=df.loc[df[cluster_column].isin(l_clusters)], x='norm_Times', y=cluster_column, orient='h', order=l_clusters, ax=ax1, palette=['black', 'black'], alpha=0.4)\n",
    "\n",
    "\n",
    "#sns.boxplot(data=df.loc[(df['Session'].isin(['EPM', 'OF'])) & (df['Exclude'] == False)], x='Times', y=cluster_column, orient='h', order=l_clusters, ax=ax1, palette='husl')\n",
    "#if stripplot:\n",
    "#    sns.stripplot(data=df.loc[(df['Session'].isin(['EPM', 'OF'])) & (df['Exclude'] == False)], x='Times', color='k', y=cluster_column, orient='h', order=l_clusters, ax=ax1)\n",
    "#plt.title('EPM & OF', fontsize=20)\n",
    "plt.ylabel('Cluster ID')\n",
    "plt.xlim(0,1)\n",
    "plt.xlabel('normalized recording time')\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "plt.savefig('Cont_run-2_normalized_recording_times_all_data.png', dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-concentration",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-edgar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = 'cont'\n",
    "run = '2'\n",
    "\n",
    "measure_columns = ['norm_HeartRate', 'norm_HR_CoV_10s', 'norm_Motion', 'norm_AreaExplored_sqrt', 'norm_Temperature_s1']\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,5), facecolor='white')\n",
    "\n",
    "gs = fig.add_gridspec(1, 1)\n",
    "\n",
    "\n",
    "l_dfs_individual_clusters = []\n",
    "\n",
    "for cluster_id in l_clusters:\n",
    "    l_dfs_single_measurements = []\n",
    "    for measure in measure_columns:\n",
    "        df_temp = df.loc[df['Cluster_{}_r{}'.format(code, run)] == cluster_id, ['Data_idx', 'Animal_ID', measure]].copy()\n",
    "        df_temp.columns = ['Data_idx', 'Animal_ID', 'Data']\n",
    "        n_datapoints = df_temp.shape[0]\n",
    "        N_mice = df_temp['Animal_ID'].unique().shape[0]\n",
    "        df_temp['Measure'] = measure\n",
    "        df_temp['Cluster_Run'] = 'Cluster-{}_Run-{} ({} datapoints from {} mice)'.format(str(cluster_id), run, n_datapoints, N_mice)\n",
    "        l_dfs_single_measurements.append(df_temp)\n",
    "    l_dfs_individual_clusters.append(pd.concat(l_dfs_single_measurements))\n",
    "\n",
    "df_for_boxplots = pd.concat(l_dfs_individual_clusters)\n",
    "\n",
    "ax = fig.add_subplot(gs[0,0])\n",
    "\n",
    "sns.boxplot(x=\"Measure\", y=\"Data\", hue=\"Cluster_Run\", data=df_for_boxplots, palette=['magenta', 'darkgreen'])\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.get_legend().remove()\n",
    "\n",
    "plt.ylabel('normalized measure')\n",
    "plt.ylim(0,1)\n",
    "\n",
    "#plt.savefig('Cont_run-2_boxplots_120-magenta_43_green.png', dpi=600)\n",
    "plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-adelaide",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pie_chart_infos(cluster_id):\n",
    "    df_temp = df_inspect.loc[df_inspect[cluster_column] == cluster_id, ['behaviors', 'Bin']].copy()\n",
    "    total_bins = df_temp.shape[0]\n",
    "    behaviors_perc = {'Immobility': round(df_temp.loc[df_temp['behaviors'] == 'Immobility'].shape[0] / total_bins * 100, 2),\n",
    "                      'Stretch Attend': round(df_temp.loc[df_temp['behaviors'] == 'StretchAttend'].shape[0] / total_bins * 100, 2),\n",
    "                      'No score': round(df_temp.loc[df_temp['behaviors'] == 'No score'].shape[0] / total_bins * 100, 2),\n",
    "                      'Grooming': round(df_temp.loc[df_temp['behaviors'] == 'Grooming'].shape[0] / total_bins * 100, 2),\n",
    "                      'Remaining': round(df_temp.loc[df_temp['behaviors'] == 'Remaining'].shape[0] / total_bins * 100, 2),\n",
    "                      'Tail Rattling': round(df_temp.loc[df_temp['behaviors'] == 'TailRattling'].shape[0] / total_bins * 100, 2),\n",
    "                      'Flight': round(df_temp.loc[df_temp['behaviors'] == 'Flight'].shape[0] / total_bins * 100, 2),\n",
    "                      'Rearing': round(df_temp.loc[df_temp['behaviors'] == 'Rearing'].shape[0] / total_bins * 100, 2),\n",
    "                      \n",
    "                      #'Struggle': round(df_temp.loc[df_temp['behaviors'] == 'Struggle'].shape[0] / total_bins * 100, 2),\n",
    "                      #'OpenRearing': round(df_temp.loc[df_temp['behaviors'] == 'OpenRearing'].shape[0] / total_bins * 100, 2),\n",
    "                      'HeadDips': round(df_temp.loc[df_temp['behaviors'] == 'HeadDips'].shape[0] / total_bins * 100, 2),\n",
    "                      \n",
    "                      'Multiple': round(df_temp.loc[df_temp['behaviors'] == 'multiple'].shape[0] / total_bins * 100, 2)}\n",
    "    labels = []\n",
    "    sizes = []\n",
    "    for key in behaviors_perc.keys():\n",
    "        if behaviors_perc[key] > 1:\n",
    "            labels.append(key)\n",
    "            sizes.append(behaviors_perc[key])\n",
    "    return sizes, labels\n",
    "\n",
    "def calculate_pie_chart_infos_average():\n",
    "    df_temp = df.copy()\n",
    "    total_bins = df_temp.shape[0]\n",
    "    behaviors_perc = {'Immobility': round(df_temp.loc[df_temp['behaviors'] == 'Immobility'].shape[0] / total_bins * 100, 2),\n",
    "                      'Stretch Attend': round(df_temp.loc[df_temp['behaviors'] == 'StretchAttend'].shape[0] / total_bins * 100, 2),\n",
    "                      'No score': round(df_temp.loc[df_temp['behaviors'] == 'No score'].shape[0] / total_bins * 100, 2),\n",
    "                      'Grooming': round(df_temp.loc[df_temp['behaviors'] == 'Grooming'].shape[0] / total_bins * 100, 2),\n",
    "                      'Remaining': round(df_temp.loc[df_temp['behaviors'] == 'Remaining'].shape[0] / total_bins * 100, 2),\n",
    "                      'Tail Rattling': round(df_temp.loc[df_temp['behaviors'] == 'TailRattling'].shape[0] / total_bins * 100, 2),\n",
    "                      'Flight': round(df_temp.loc[df_temp['behaviors'] == 'Flight'].shape[0] / total_bins * 100, 2),\n",
    "                      'Rearing': round(df_temp.loc[df_temp['behaviors'] == 'Rearing'].shape[0] / total_bins * 100, 2),\n",
    "                      \n",
    "                      #'OpenRearing': round(df_temp.loc[df_temp['behaviors'] == 'OpenRearing'].shape[0] / total_bins * 100, 2),\n",
    "                      #'Struggle': round(df_temp.loc[df_temp['behaviors'] == 'Struggle'].shape[0] / total_bins * 100, 2),\n",
    "                      'HeadDips': round(df_temp.loc[df_temp['behaviors'] == 'HeadDips'].shape[0] / total_bins * 100, 2),\n",
    "                      \n",
    "                      'Multiple': round(df_temp.loc[df_temp['behaviors'] == 'multiple'].shape[0] / total_bins * 100, 2)}\n",
    "    labels = []\n",
    "    sizes = []\n",
    "    for key in behaviors_perc.keys():\n",
    "        if behaviors_perc[key] > 0.5:\n",
    "            labels.append(key)\n",
    "            sizes.append(behaviors_perc[key])\n",
    "    return sizes, labels\n",
    "        \n",
    "fig = plt.figure(figsize=(22, 10), facecolor='w')\n",
    "gs = fig.add_gridspec(1,3)\n",
    "plt.subplots_adjust(hspace=1) \n",
    "\n",
    "cluster_id = 0\n",
    "\n",
    "\n",
    "for column in [0, 1]:\n",
    "    fig.add_subplot(gs[0,column])\n",
    "    sizes, labels = calculate_pie_chart_infos(l_clusters[cluster_id])\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "            shadow=False, startangle=90, rotatelabels=True, textprops={'fontsize': 16})\n",
    "    plt.axis('equal')\n",
    "    plt.title('Cluster #' + str(l_clusters[cluster_id]) + '\\n' + str(df_all_clusters.loc[l_clusters[cluster_id],'Cluster_size']) + ' bins', pad=10, fontsize=24)\n",
    "    cluster_id = cluster_id + 1\n",
    "\n",
    "fig.add_subplot(gs[0,2])\n",
    "sizes, labels = calculate_pie_chart_infos_average()\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=False, startangle=90, rotatelabels=True, textprops={'fontsize': 16})\n",
    "plt.axis('equal')\n",
    "plt.title('Entire DataFrame \\n' + str(df.shape[0]) + ' bins', pad=10, fontsize=24)\n",
    "cluster_id = cluster_id + 1\n",
    "\n",
    "plt.savefig('Cont_run-2_pie_charts_120-43.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_run_2 = [elem[1][1] for elem in l_matching_triplets_abc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 'UMAP_cont_r2_1'\n",
    "Y = 'UMAP_cont_r2_2'\n",
    "hue = 'Cluster_cont_r2'\n",
    "\n",
    "palette = 'viridis'\n",
    "\n",
    "size = 2\n",
    "\n",
    "fig = plt.figure(figsize=(20, 15), facecolor='w')\n",
    "gs = fig.add_gridspec(1,1)\n",
    "\n",
    "ax = fig.add_subplot(gs[0,0])\n",
    "sns.scatterplot(\n",
    "    x=X,\n",
    "    y=Y,\n",
    "    #hue=hue,\n",
    "    palette=palette,\n",
    "    data=df,\n",
    "    legend=False,\n",
    "    alpha=0.3,\n",
    "    s=size)\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=X,\n",
    "    y=Y,\n",
    "    color='gold',\n",
    "    data=df.loc[df[hue].isin(clusters_run_2)],\n",
    "    legend=False,\n",
    "    alpha = 0.3,\n",
    "    s=size)\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=X,\n",
    "    y=Y,\n",
    "    color='magenta',\n",
    "    data=df.loc[df[hue] == 120],\n",
    "    legend=False,\n",
    "    s=size+1)\n",
    "\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=X,\n",
    "    y=Y,\n",
    "    color='darkgreen',\n",
    "    data=df.loc[df[hue] == 43],\n",
    "    legend=False,\n",
    "    s=size+1)\n",
    "\n",
    "\n",
    "plt.xlim(-15,15)\n",
    "plt.ylim(-15,15)\n",
    "\n",
    "# Hide the right and top spines\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Only show ticks on the left and bottom spines\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "#plt.setp(ax.spines.values(), linewidth=2)\n",
    "\n",
    "#plt.title('Global scaling - 87 nn - Type 3 - factor 1')\n",
    "plt.xlabel('UMAP dimension one')\n",
    "plt.ylabel('UMAP dimension two')\n",
    "\n",
    "#plt.savefig('Cont_run-2_no_cluster.png', dpi=600)\n",
    "\n",
    "plt.savefig('Cont_run-2_magenta-120_green-43_only_stable_clusters.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-ordinance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_measure_columns = {'intv_100': [column for column in df.columns if column.endswith('_intervals_100')],\n",
    "                     'cont': [column[:column.index('_intervals_100')] for column in df.columns if column.endswith('_intervals_100')]}\n",
    "\n",
    "l_dfs_individual_clusters = []\n",
    "for elem in l_run_cluster_combis:\n",
    "    run, cluster_id = elem\n",
    "    l_dfs_single_measurements = []\n",
    "    for measure in d_measure_columns[code]:\n",
    "        df_temp = df.loc[df['Cluster_{}_r{}'.format(code, run)] == cluster_id, ['Data_idx', measure]].copy()\n",
    "        df_temp.columns = ['Data_idx', 'Data']\n",
    "        df_temp['Measure'] = measure\n",
    "        df_temp['Cluster_Run'] = 'Cluster-{}_Run-{}'.format(str(cluster_id), run)\n",
    "        l_dfs_single_measurements.append(df_temp)\n",
    "    l_dfs_individual_clusters.append(pd.concat(l_dfs_single_measurements))\n",
    "\n",
    "df_for_boxplots = pd.concat(l_dfs_individual_clusters)\n",
    "\n",
    "plt.figure(figsize=(20,5), facecolor='white')\n",
    "\n",
    "sns.boxplot(x=\"Measure\", y=\"Data\", hue=\"Cluster_Run\", data=df_for_boxplots, palette=\"Set3\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cluster_cont_r3'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-contract",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-texture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
